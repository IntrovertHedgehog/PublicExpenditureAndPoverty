---
title: "Public Expenditure and Poverty"
author: "Team 1: Le Van Minh, Callista Stephine Yu, Muhammad Imtiyaaz B M A M , Thum Jun Long Issac"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
bibliography: citations.bib
urlcolor: blue
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE, warning = F)
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 50), tidy = TRUE)
# inline hook to 2 digits
inline_hook <- function(x) {
    if(is.numeric(x)) x <- round(x, 2)
    paste(as.character(x), collapse=", ")
}
knit_hooks$set(inline = inline_hook)
# code chunk out put to 
options(digits = 3)
```

# 1. Introduction
The project aim is to explore different models using the same dataset to evaluate which method will be the best to build a model for the dependent variable. The dataset we are using for this project has 1901 data values from 23 different variables. Our plan for this project is to first build a model with all the variables in the dataset. We will then follow it up by builidng a model without the missing values and build a ridge regression and Lasso regression model. Our second approach will be to selectively remove variables from the model and to build the best model  using regularisation and regression. Our next approach was to add the variables into the model one at a time to see if a different model can be achieved. Lastly, we performed imputation on our model to tackle the missing values present in our dataset.We then compared all of this approaches to come to a conclusion.

Github repo: [https://github.com/IntrovertHedgehog/PublicExpenditureAndPoverty](https://github.com/IntrovertHedgehog/PublicExpenditureAndPoverty)

# 2. Data Characteristic

```{r, message = F}
library(needs)

needs("dplyr", "MASS", "readr", "tidyr", "ggplot2", "e1071", "moments", "corrplot", "Hmisc", "PerformanceAnalytics", "mice", "car", "glmnet", "ggforce", "lmtest")

prioritize(dplyr)
```

## 2.1. Nature of Data

The data set is collection The World Bank Data, the variables of interest are extracted from the raw data files and combined into a single data frame for analysis. The final data set includes:

1.  **country.code**: Country code\
2.  **country.name**: Country name\
3.  **year**: Year\
4.  **income**: Income class
-   Low income (L)\
-   Lower middle income (LM)\
-   Upper middle income (UM)\
-   High income (H)\
5.  **reg**: Region\
6.  **pov**: Poverty headcount ratio based on cut-off value of $2.15 per day\
7.  **mpi**: Multidimensional Poverty Index\
8.  **edu.total**: Total expenditure on education (% of GDP)\
9.  **edu.pri**: Total expenditure on primary education (% of total education expenditure)\
10. **edu.sec**: Total expenditure on secondary education (% of total education expenditure)\
11. **edu.ter**: Total expenditure on tertiary education (% of total education expenditure)\
12. **hlth**: Total expenditure on health (% of GDP)\
13. **mil**: Total expenditure on military (% of GDP)\
14. **fdi**: Foreign Direct Investment\
15. **lbr.part**: Labour force participation (% of population ages 15+)\
16. **unemp**: Unemployment rate\
17. **pop.gwth.total**: Total population growth rate\
18. **pop.gwth.rural**: Total rural population growth rate\
19. **pop.gwth.urban**: Total urban population growth rate\
20. **gdp.dflt**: GDP deflator\
21. **gdr.eql**: Gender equality rating\
22. **gcf**: Gross Capital Formation\
23. **trade**: Trade = import + export (% of GDP)\
24. **gdp.pc**: GDP per capita (current US$)

Data imports and combining:

```{r}
# helper functions
importWDI <- function(filepath, value_name) {
  df <- read_csv(filepath, skip = 4) 
  
  colnames(df) <- tolower(gsub(" ", ".", colnames(df)))
  
  df <- df %>% 
    pivot_longer(5:ncol(.), names_to = "year", values_to = "value") %>% 
    filter(!is.null(value) & !is.na(value)) %>% 
    mutate(country.code = factor(country.code), 
           country.name = factor(country.name),
           year = as.numeric(year)) %>% 
    select(country.code, country.name, year, value)
  
  colnames(df)[4] <- value_name
  
  df
}

importRegionClass <- function(filepath) {
  df <- read_csv(filepath, skip = 4) 
  
  colnames(df) <- tolower(gsub(" ", ".", colnames(df)))
  
  df %>% mutate(country.name = factor(country.name),
                region = factor(region)) %>% 
    select(country.name, reg = region)
}

importIncomeClass <- function(filepath) {
  df <- read_csv(filepath, skip = 4) 
  
  colnames(df) <- tolower(gsub(" ", ".", colnames(df)))
  
  df %>% 
    pivot_longer(3:ncol(.), names_to = "year", values_to = "income") %>% 
    filter(!is.null(income) & !is.na(income)) %>% 
    mutate(country.code = factor(country.code), 
           country.name = factor(country.name), 
           year = as.numeric(year), 
           income = factor(income)) %>% 
    select(country.code, country.name, year, income)
}
```

```{r, message = F}
# import data
setwd("../data")

poverty.headcount <- importWDI("poverty.headcount.215dollar.csv", "pov")
mpi <- importWDI("mpi.csv", "mpi")
education.expenditure.total <- importWDI("total.education.expenditure.csv", "edu.total")
education.expenditure.primary <- importWDI("primary.education.expenditure.csv", "edu.pri")
education.expenditure.secondary <- importWDI("secondary.education.expenditure.csv", "edu.sec")
education.expenditure.tertiary <- importWDI("tertiary.education.expenditure.csv", "edu.ter")
health.expenditure <- importWDI("health.expenditure.csv", "hlth")
military.expenditure <- importWDI("military.expenditure.csv", "mil")
fdi <- importWDI("fdi.csv", "fdi")
labour.force.participation <- importWDI("labour.force.participation.csv", "lbr.part")
unemployment.rate <- importWDI("unemployment.csv", "unemp")
population.growth <- importWDI("population.growth.csv", "pop.gwth.total")
rural.population.growth <- importWDI("rural.population.growth.csv", "pop.gwth.rural")
urban.population.growth <- importWDI("urban.population.growth.csv", "pop.gwth.urban")
gdp.deflator <- importWDI("gdp.deflator.csv", "gdp.dflt")
gender.equality <- importWDI("gender.equality.csv", "gdr.eql")
gross.capital.formation <- importWDI("gross.capital.formation.csv", "gcf")
trade <- importWDI("trade.csv", "trade")
region.class <- importRegionClass("region.class.csv")
income.class <- importIncomeClass("income.class.csv")
gdp.pc <- importWDI("gdp.pc.csv", "gdp.pc")

setwd("../src")
```

We found that the data sets collected from [World Bank's Data helpdesk](https://datahelpdesk.worldbank.org/knowledgebase/articles/906519-world-bank-country-and-lending-groups) and [The World Bank's Data](https://data.worldbank.org/) have different naming convention for certain countries (e.g. "Czechia" vs. "Czechnia Republic"). So we need to rename these countries to avoid some error when joining.

Furthermore, WDI's data sets rate also account for non-country (e.g. country.name = "Low income" or "South Asia"). These special groups are not in our scope of interest, which is national, so we eliminate them.

```{r}
# using poverty.headcount as a naming standard (as other data from WDI also use this convention)
# join a subset of data to process the names
d <- poverty.headcount %>%
  select(country.name) %>%
  mutate(inPov = T) %>%  
  full_join(income.class %>% 
              select(country.name) %>% 
              mutate(inIncome = T), by = "country.name") %>%  
  full_join(region.class %>%
              select(country.name) %>% 
              mutate(inReg = T), by = "country.name") %>% 
  mutate(inPov = !is.na(inPov), inIncome = !is.na(inIncome), inReg = !is.na(inReg))

d
```

First, remove special economic groups from `poverty.headcount`. We figured these regions will not appear in `income.class` or `region.class`, so we might find something from looking at the countries **only** appear in `poverty.headcount`.

```{r}
d %>% filter(inPov & (!inIncome | !inReg)) %>% distinct(country.name)
```

Lucky! We can look through these 18 results and compose a list of special regions.

```{r}
spec.reg <- c("Fragile and conflict affected situations", "IDA total", "World", "East Asia & Pacific", "Europe & Central Asia", "Latin America & Caribbean", "Middle East & North Africa", "South Asia", "Sub-Saharan Africa", "Low income", "Low & middle income", "Lower middle income", "Upper middle income", "High income")
```

Then, we rename those countries with inconsistent naming convention. Since we should only care about countries whose poverty headcount is available, reusing the list generated above, we can identify:

1.  Cote d'Ivoire (also Côte d'Ivoire)
2.  Czechia (also Czechoslovakia or Czech Republic)
3.  Curacao (also Curaçao)
4.  Turkiye (formerly known as Turkey, also Türkiye)
5.  Sao Tome and Principe (also São Tomé and Príncipe)

```{r}
# mapping standard name and variation
nameMap <- tibble(standard = c("Cote d'Ivoire", "Czechia", "Czechia", "Curacao", "Turkiye", "Turkiye", "Sao Tome and Principe"), variation = c("Côte d'Ivoire", "Czechoslovakia", "Czech Republic", "Curaçao", "Turkey", "Türkiye", "São Tomé and Príncipe"))

correctName <- function(name) {
  tibble(name = name) %>% 
    left_join(nameMap, by = c("name" = "variation")) %>% 
    mutate(standard = ifelse(is.na(standard), name, standard)) %>% 
    select(standard) %>% 
    pull()
}

orig.name <- c("Vietnam", "China", "Turkey", "Czechia Republic")
correctName(orig.name)
```

Let's test this out!

```{r}
d <- poverty.headcount %>%
  # correct name here
  mutate(country.name = correctName(country.name)) %>% 
  select(country.name) %>%
  mutate(inPov = T) %>%
  full_join(income.class %>% 
              # correct name here
              mutate(country.name = correctName(country.name)) %>% 
              select(country.name) %>% 
              mutate(inIncome = T), by = "country.name") %>%  
  full_join(region.class %>%
              # correct name here
              mutate(country.name = correctName(country.name)) %>% 
              select(country.name) %>% 
              mutate(inReg = T), by = "country.name") %>% 
  filter(!(country.name %in% spec.reg)) %>% 
  mutate(inPov = !is.na(inPov), inIncome = !is.na(inIncome), inReg = !is.na(inReg))

# countries not in region list, but is in Pov list
d %>% 
  filter(!inReg & inPov) %>% 
  distinct(country.name) %>% 
  nrow()
# countries not in income list, but is in Pov list
d %>% 
  filter(!inIncome & inPov) %>% 
  distinct(country.name) %>% 
  nrow()
```

We are *pretty* confident that there's no inconsistent naming left unprocessed in the data sets.

```{r}
# Rename countries in all data sets.
poverty.headcount <- poverty.headcount %>%
  mutate(country.name = correctName(country.name))
mpi <- mpi %>%
  mutate(country.name = correctName(country.name))
education.expenditure.total <- education.expenditure.total %>%
  mutate(country.name = correctName(country.name))
education.expenditure.primary <- education.expenditure.primary %>%
  mutate(country.name = correctName(country.name))
education.expenditure.secondary <- education.expenditure.secondary %>%
  mutate(country.name = correctName(country.name))
education.expenditure.tertiary <- education.expenditure.tertiary %>%
  mutate(country.name = correctName(country.name))
health.expenditure <- health.expenditure %>%
  mutate(country.name = correctName(country.name))
military.expenditure <- military.expenditure %>%
  mutate(country.name = correctName(country.name))
fdi <- fdi %>%
  mutate(country.name = correctName(country.name))
labour.force.participation <- labour.force.participation %>%
  mutate(country.name = correctName(country.name))
unemployment.rate <- unemployment.rate %>%
  mutate(country.name = correctName(country.name))
population.growth <- population.growth %>%
  mutate(country.name = correctName(country.name))
rural.population.growth <- rural.population.growth %>%
  mutate(country.name = correctName(country.name))
urban.population.growth <- urban.population.growth %>%
  mutate(country.name = correctName(country.name))
gdp.deflator <- gdp.deflator %>%
  mutate(country.name = correctName(country.name))
gender.equality <- gender.equality %>%
  mutate(country.name = correctName(country.name))
gross.capital.formation <- gross.capital.formation %>%
  mutate(country.name = correctName(country.name))
trade <- trade %>%
  mutate(country.name = correctName(country.name))
region.class <- region.class %>%
  mutate(country.name = correctName(country.name))
income.class <- income.class %>%
  mutate(country.name = correctName(country.name))
gdp.pc <- gdp.pc %>%
  mutate(country.name = correctName(country.name))
```

Join the data

```{r, message = F}
countries <- poverty.headcount %>% 
  # We used a full join here so we can conduct a separate analysis on mpi later
  full_join(mpi, by = c("country.name", "country.code", "year")) %>%
  left_join(income.class, c("country.name", "country.code", "year")) %>% 
  left_join(region.class, by = "country.name") %>%
  left_join(education.expenditure.total, by = c("country.name", "country.code", "year")) %>%
  left_join(education.expenditure.primary, by = c("country.name", "country.code", "year")) %>%
  left_join(education.expenditure.secondary, by = c("country.name", "country.code", "year")) %>%
  left_join(education.expenditure.tertiary, by = c("country.name", "country.code", "year")) %>%
  left_join(health.expenditure, by = c("country.name", "country.code", "year")) %>%
  left_join(military.expenditure, by = c("country.name", "country.code", "year")) %>%
  left_join(fdi, by = c("country.name", "country.code", "year")) %>%
  left_join(labour.force.participation, by = c("country.name", "country.code", "year")) %>%
  left_join(unemployment.rate, by = c("country.name", "country.code", "year")) %>%
  left_join(population.growth, by = c("country.name", "country.code", "year")) %>%
  left_join(rural.population.growth, by = c("country.name", "country.code", "year")) %>%
  left_join(urban.population.growth, by = c("country.name", "country.code", "year")) %>%
  left_join(gdp.deflator, by = c("country.name", "country.code", "year")) %>%
  left_join(gender.equality, by = c("country.name", "country.code", "year")) %>%
  left_join(gross.capital.formation, by = c("country.name", "country.code", "year")) %>%
  left_join(trade, by = c("country.name", "country.code", "year")) %>% 
  left_join(gdp.pc, by = c("country.name", "country.code", "year")) %>% 
  # filter special groups
  filter(!(country.name %in% spec.reg))
```

Data preview

```{r}
head(countries)
str(countries)
summary(countries)
```

There's no NA in `reg`, which is a sign that all naming in the data is remedied. There's some expected NAs in `income` and `pov`, as these data are collected by year. There's a substantial amount of missing data in `mpi`, as this is a relative new concept. We will address the nature, and processing of missing data in the next sections.

## 2.2. Missing values

As observed from the summary above, the data set contains a lot of missing values in some of the variables.

```{r}
mean(is.na(countries))
```

About 19% of the data set is missing.

```{r}
nCompleteObs <- sum(complete.cases(countries))
print(paste("No. of complete cases:", nCompleteObs))
```

There are only 3 complete cases where all the variable is available. This is nowhere near acceptable to conduct any meaningful analysis. Therefore, we need to eliminate some variables for a more balance data set.

```{r}
missings <- colMeans(is.na(countries))
ggplot(mapping = aes(x = names(missings), y = missings, fill = missings < 0.35)) +
  geom_bar(stat = "identity") +
  ggtitle("Missing rate") +
  xlab("variables") +
  ylab("% missing") +
  theme(axis.text.x = element_text(size=9, angle=90))

missings[missings > 0.35]
```

There are 5 variables with missing rate \>35%.

expenditure in primary, secondary, and tertiary edication can be very useful and relevant information to predict poverty reduction [@pubspend&pov]. However, we would like to exclude these variables from some first analyses to make use of the richer set of data. We can conduct a separate analysis with these variable to gain more insight.

```{r}
# variables with high missing rate
hMiss <- names(missings[missings > 0.35])
# exclude these variables in countries1
countries1 <- countries %>% 
  select(!hMiss) %>% 
  filter(!is.na(pov))
str(countries1)
```

Re-evaluate the `countries1` set.

```{r}
mean(is.na(countries1))
sum(complete.cases(countries1))
mean(complete.cases(countries1))
```

On average, each column has 6% missing rate, results in 937 complete data point (i.e. 49%). This can be a sufficient number for the analysis. However, the missing data can induce loss of power due to the reduced sample size, and some other biases depending on which variables is missing.

```{r}
# complete rate of data by regions
countries1 %>% 
  mutate(isComplete = complete.cases(.)) %>% 
  group_by(reg) %>% 
  summarise(complete.rate = mean(isComplete)) %>% 
  arrange(desc(complete.rate))
```

Countries from North America, Sub-Saharan Africa, and South Asia have the highest rate of missing data. We suspect that Sub-Saharan Africa, and South Asia are comparably less accessible regions. We also know that Americans don't like filling out forms, so their high rate of missing data is understandable as well.

Still, we need to find a way to address this issue. we propose several approaches:

1.  **Use complete cases**: Only use the complete cases for the analysis. This is a straightforward approach, but doesn't resolve the bias resulted from the massive loss of data.
2.  **Selectively remove variables with high missing rate**: Start with all the variable in the data set, then step-by-step remove insignificant, or frequently missing variables to use as much data as possible.
3.  **Forward variable selection**: Start with 0 variable, then selectively add potentially important variable to the data set to achieve optimal model.
4.  **Imputation**: The idea is to replace the missing observations on the response or the predictors with artificial values that try to preserve the data set structure. This is a quite complex topic of its own. You can read more from @imputation.

## 2.3. Descriptive Analytics
Distribution of the predicted variable `pov`
```{r}
ggplot(countries, aes(x = pov)) +
  geom_histogram(aes(y = ..density..), bins = 30) +
  geom_density(alpha = .2, fill = "blue")
```
The graph displays a decreasing rate as poverty indicator increasing. This might not be representative of the current state of poverty in the world, but of the number presented in our data. For example, more recent data is likely to be more inclusive than ancient data, when poverty is more prevalent. We should look at data from the same period.

```{r}
# number of data point available from 1967 to 2021
ggplot(poverty.headcount, aes(x = year)) +
  geom_histogram(bins = 30)

# pov data from 1998 and 2018
pov.98.18 <- poverty.headcount %>%
  filter(year == 1998 | year == 2018)
pov.98.18 %>% 
  group_by(year) %>% 
  summarise(sum = n())

ggplot(pov.98.18, aes(x = pov)) +
  geom_histogram(aes(y = ..density..), bins = 30) +
  geom_density(alpha = .2, fill = "blue") +
  facet_grid(cols = vars(year))
```

The graph for 1998 has a much gentler slope, meaning poverty was more popular during that time, as predicted from out intuition. What about the general progress of the world?

```{r}
# Re-import pov and only take special regions
# geographic
geo.regs <- c("EAS", "ECS", "LCN", "MEA", "SAS", "SSF", "WLD")
# economics
eco.regs <- c("HIC", "LIC", "LMC", "LMY", "UMC")

pov.reg <- importWDI("../data/poverty.headcount.215dollar.csv", "pov") %>% 
  filter(country.code %in% c(geo.regs, eco.regs)) %>% 
  mutate(type = ifelse(country.code %in% geo.regs, "Geographic", "Economics"))

pov.reg %>%
  distinct(country.code, country.name, type) %>% 
  arrange(type)

# World
ggplot(pov.reg %>% filter(country.code == "WLD"),
       aes(x = year, y = pov)) +
  geom_point() +
  geom_smooth(formula = y~x, method = loess)
```

An overall very steady decrease of poverty. How about each region?

```{r}
ggplot(pov.reg %>% filter(country.code != "WLD"),
       aes(x = year, y = pov, color = country.name)) +
  geom_point() +
  geom_smooth(formula = y~x, method = loess) +
  facet_grid(rows = vars(type))
```

There's a general steady, but distinct decline of poverty over time in each type region of respective type. Latin America & Caribbean, Europe & Central Asia, Middle East & North Africa, and High Income group has a more gradual decline as they are not very poor to begin with.  

Among the income groups, Low & Middle Income, Lower & Middle Income, and Upper Middle Income have quite similar in term of poverty indicator and slope over the year. While these values vary greatly among different geographical regions.  

Let's see some important statistics

```{r}
stats <- poverty.headcount %>% 
  summarise(count = n(), skewness = skewness(pov), kurtosis = kurtosis(pov), std.deviation = sd(pov))

kable(stats)
```

## 2.4. Assumption Check
We can conduct some preliminary checks on linearity and correlation of predictors to have a better picture of the data. [Checklist](https://godatadrive.com/blog/basic-guide-to-test-assumptions-of-linear-regression-in-r)
**1. Linear relationship:**  
Use correlation matrix to check linearity
```{r, warning = FALSE}
# select only numeric data
countries.num <- countries %>% select(where(is.numeric))
chart.Correlation(countries.num, histogram = TRUE, pch = 19)
```
Which is utterly intelligible, but a majority of the fitted lines are linear at first glance. We should render separate graphs for the relationship between `pov` & other variables.
```{r}
# lengthen data table to variable-value pairs, with pov as predicted variable and others as predictive 
countries.num2 <- countries.num %>% 
  pivot_longer(cols = 3:ncol(.), names_to = "variable", values_to = "value") %>% 
  filter(!is.na(value) & !is.na(pov))

drawGraph <- function(indvar, data) {
  ggplot(data %>% 
           filter(variable == indvar),
         aes(x = value, y = pov)) +
    geom_point() +
    geom_smooth(method = lm) +
    labs(title = paste("Relationship pov ~", indvar),
         x = indvar,
         y = "pov")
}

ivs <- distinct(countries.num2, variable)[[1]]

# for (indvar in ivs) {
#   print(
#     ggplot(countries.num2 %>% 
#            filter(variable == indvar),
#          aes(x = value, y = pov)) +
#     geom_point() +
#     geom_smooth(formula = y~x, method = lm) +
#     labs(title = paste("Relationship pov ~", indvar),
#          x = indvar,
#          y = "pov")
#   )
# }

p <- ggplot(countries.num2, 
     aes(x = value, y = pov)) +
geom_point() +
geom_smooth(formula = y~x, method = lm) +
labs(title = paste("pov ~ variable"),
     x = "variable",
     y = "pov") +
facet_wrap_paginate(~ variable,
                    ncol = 3,
                    nrow = 1,
                    scales = "free")

requiredPages <- n_pages(p)

for (i in 1:requiredPages) {
  ggplot(countries.num2,
         aes(x = value, y = pov)) +
  geom_point() +
  geom_smooth(formula = y~x, method = lm) +
  labs(title = paste("pov ~ variable"),
       x = "variable",
       y = "pov") +
  facet_wrap_paginate(~ variable,
                      ncol = 3,
                      nrow = 1,
                      scales = "free",
                      page = i) -> p
  print(p)
}
```

Most variables display linear relationship with some exceptions, which are more appropriate to assume a logarithmic relationship. Gender equality should be viewed as a categorical data.
```{r}
logIvs <- c("fdi", "gdp.dflt", "gdp.pc")

for (indvar in logIvs) {
  print(
    ggplot(countries.num2 %>% 
           filter(variable == indvar),
         aes(x = log(value), y = pov)) +
    geom_point() +
    geom_smooth(formula = y ~ x, method = lm) +
    labs(title = paste0("pov ~ log(", indvar, ")"),
         x = paste0("log(", indvar, ")"),
         y = "pov")
  )
}
```

The relationship are more linear after the transformation.

```{r}
gdr.eql <- countries.num2 %>% 
  filter(variable == "gdr.eql") %>% 
  mutate(value = factor(value))

ggplot(gdr.eql,
       aes(x = value, y = pov)) +
  geom_boxplot() +
  labs(title = paste("pov ~ gdr.eql"),
       x = "gdr.eql",
       y = "pov")
```
There's a significant correlation between gender equality and poverty.  
Correlation coefficients between `pov` and predictors.

```{r}
# transform some variables
countries.num3 <- countries.num %>% 
  mutate(lfdi = log(fdi), lgdp.dflt = log(gdp.dflt), lgdp.pc = log(gdp.pc)) %>% 
  mutate(lfdi = ifelse(is.nan(lfdi) | lfdi == -Inf, NA, lfdi))

# name of the independent variables
cn <- colnames(countries.num3)[-c(1,2)]

# correlation of independent variable and pov
corWithPov <- function(indevar, data) {
  cor(data[[indevar]], data[["pov"]], use = "complete.obs")
}

cor.pov <- sapply(cn, corWithPov, data = countries.num3)
kable(cor.pov)
```

`edu.ter`, `mil`, `fdi`, `lbr.part`, `unemp`, `gdp.dflt`, `gcf` have negligible correlation with `pov`. `lgdp.pc`, `lfdi`, and `lgdp.dflt` have stronger linear relationship with `pov` than their un-transformed counterparts.

```{r}
countries1 <- countries1 %>% 
  mutate(lfdi = log(fdi), lgdp.dflt = log(gdp.dflt), lgdp.pc = log(gdp.pc)) %>% 
  mutate(lfdi = ifelse(is.nan(lfdi) | lfdi == -Inf, NA, lfdi))
```


**2. Predictors are independent**
There should be no correlation/multicolinearity between each pair of predictors.

```{r}
countries.arr <- simplify2array(countries.num %>% 
                                  select(-c("year", "pov")))

cor.mtx <- rcorr(countries.arr, type = "spearman")

round(cor.mtx$r, 2)

corrplot(cor.mtx$r, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)
```
Some significant correlations can be found between `gdr.eql` with `edu.pri`, `pop.gwth.total`, and `fdi` with `mil`, etc. We expect these variables to be eliminated in the VIF test. This might be a good property for imputation (3.2.4)


## 2.5. Data Source

- [poverty.headcount](https://data.worldbank.org/indicator/SI.POV.DDAY)
- [mpi](https://data.worldbank.org/indicator/SI.POV.MDIM.XQ)
- [education.expenditure.primary](https://data.worldbank.org/indicator/SE.XPD.PRIM.ZS)
- [education.expenditure.secondary](https://data.worldbank.org/indicator/SE.XPD.SECO.ZS)
- [education.expenditure.tertiary](https://data.worldbank.org/indicator/SE.XPD.TERT.ZS)
- [education.expenditure.total](https://data.worldbank.org/indicator/SE.XPD.TOTL.GD.ZS)
- [health.expenditure](https://data.worldbank.org/indicator/SH.XPD.GHED.GD.ZS)
- [military.expenditure](https://data.worldbank.org/indicator/MS.MIL.XPND.GD.ZS)
- [fdi](https://data.worldbank.org/indicator/BX.KLT.DINV.CD.WD)
- [unemployment.rate](https://data.worldbank.org/indicator/SL.UEM.TOTL.NE.ZS)
- [labour.force.participation](https://data.worldbank.org/indicator/SL.TLF.CACT.NE.ZS)
- [gender.equality](https://data.worldbank.org/indicator/IQ.CPA.GNDR.XQ)
- [population.growth](https://data.worldbank.org/indicator/SP.POP.GROW)
- [urban.population.growth](https://data.worldbank.org/indicator/SP.URB.GROW)
- [rural.population.growth](https://data.worldbank.org/indicator/SP.RUR.TOTL.ZG)
- [gdp.deflator](https://data.worldbank.org/indicator/NY.GDP.DEFL.KD.ZG)
- [gross capital formation](https://data.worldbank.org/indicator/NE.GDI.TOTL.ZS)
- [trade](https://data.worldbank.org/indicator/NE.TRD.GNFS.ZS)
- [region.class](https://datatopics.worldbank.org/world-development-indicators/the-world-by-income-and-region.html)
- [income.class](https://datahelpdesk.worldbank.org/knowledgebase/articles/906519-world-bank-country-and-lending-groups)
- [gross.capital.formation](https://data.worldbank.org/indicator/NE.GDI.TOTL.ZS)

# 3. Model Selection and Interpretation

We can now conduct linear regressions following the approaches mentioned above to address missing values issues.

## 3.1. Use Complete Cases
```{r}
needs("tidyverse","ggpubr","gridExtra","e1071","ggplot2","dplyr","tidyr","readxl","corrplot","car","DescTools","glmnet")
```

First we will write the function "eval_results(true,predict) to evaluate the models
```{r}
eval_results <- function(true, predict) {
  actual <- data.matrix(true)
  SSE <- sum((predict - actual)^2)
  SST <- sum((actual - mean(actual))^2)
  R_square <- 1 - SSE / SST
  data.frame(
    MSE = MSE(predict, true),
    MAE = MAE(predict, true),
    RMSE = RMSE(predict, true),
    Rsquare = R_square
  )
}
```
First we read the csv file for the countries containing the dataset. We will then put the variables we need in to subset1.
```{r}
df <-read.csv('../data/countries1.csv')
head(df)
subset1 <- df %>% select("pov","country.code","year","income","edu.total","hlth","mil","fdi","lbr.part","unemp","pop.gwth.total","pop.gwth.rural","gdp.dflt","gcf","trade","gdp.pc","lfdi","lgdp.dflt","lgdp.pc")
head(subset1)
```


Next we will build a SLR model. We will put 80% of dataset to train and the rest to test.
```{r}
set.seed(10)
index <- sort(sample(x = nrow(subset1), size = nrow(subset1)*.8))
train <- df[index, ]
test <- df[-index, ]
```

The model output will be placed in lm.slr and can be viewed in summary(lm.slr).
```{r}
lm.slr <- lm(pov ~ country.code+year+income+edu.total+hlth+mil+fdi+lbr.part+unemp+pop.gwth.total+pop.gwth.rural+gdp.dflt+gcf+trade+gdp.pc+lfdi+lgdp.dflt+lgdp.pc,data = train)
summary(lm.slr)

```
Next we will be performing standardisation.However not all the variables are numerical. Therefore to overcome it we build the function below. This will allow us to perform standardisation to the dataset.
```{r}
subset1$income <- factor(subset1$income)
subset1$country.code <- factor(subset1$country.code)



sd0 <- function(vct) {
  if (!is.numeric(vct)) {
    return(NA)
  }
  
  return(sd(vct, na.rm = T))
}


std0 <- function(vct, scl) {
  if (!is.numeric(vct)) {
    return(vct)
  }
  
  return(vct/scl)
}

scale <- function(data) {
  unlist(lapply(data, sd0))
}

standardise <- function(data) {
  scaler <- scale(data)
  
  numCols <- which(unlist(lapply(data, is.numeric)))
  num <- as.data.frame(mapply(std0, vct = data[,numCols], scl = scaler[numCols], SIMPLIFY = T))
  fct <- data[,-numCols]
  return(cbind(fct, num))
}
```

Next we will perform standardistation to the dataset.
```{r}
str(subset1)
df0 <- subset1
head(subset1)
apply(subset1, 2, sd)
```

Next we will plot a pov vs income scatter plot to see the effect of income on pov. From the scatter plot, we can deduce that the low income countries have a higher pov while the higher income countries have low pov. This is consistent with our intuition that countries that are richer do not have as much poverty issue as the poorer countries.
```{r}
ggplot(data = subset1, aes(x = income, y = pov)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(title = "Pov vs. Income ",
       x = "income",
       y = "pov")

```
Next we will build a MLR model with data from subset 1 to see the relationship between dependent variable:pov and the other variables

```{r}
model1 <- lm(pov ~ ., data = subset1)
summary(model1)
vif(model1)
```
Next we will perform regularisation. We will split the data to predictor variable "train.x" and response variables "train.y"
```{r}
subset2 <- subset(subset1,select =c("pov","country.code","year","income","edu.total","hlth","mil","fdi","lbr.part","unemp","pop.gwth.total","pop.gwth.rural","gdp.dflt","gcf","trade","gdp.pc","lfdi","lgdp.dflt","lgdp.pc"))
subset2 <- na.omit(subset2)
head(subset2)
train.x <- data.matrix(subset(subset2, select = -pov))
train.y = data.matrix(subset2$pov)
```

Next we will perform splitting of data to train and test.
```{r}
split_train_test <- function(data) {
set.seed(1984)
# split data
seeds <- data %>% 
  group_by(country.code) %>% 
  filter(row_number() == 1)

plants <- data %>% 
  group_by(country.code) %>% 
  filter(row_number() != 1)

isComplete <- which(complete.cases(plants))
idx <- sample(isComplete, replace = F, 0.2 * nrow(plants))

training <- plants[-idx,] %>% 
  rbind(seeds)
test <- plants[idx,]

return(list(training, test))
}

training <- split_train_test(subset2)[[1]]
test <- split_train_test(subset2)[[2]]

ntrain <- 1:nrow(training)
combine <- rbind(training, test)
combine.mtrx <- model.matrix(pov ~ ., data = combine)
train.mtrx <- combine.mtrx[ntrain,]
test.mtrx <- combine.mtrx[-ntrain,]

train.x <- train.mtrx[,-1]
train.y <- training$pov

test.x <- test.mtrx[,-1]
test.y <- test$pov
```

We then build a MLR model with the training model to train it.

```{r}
model3 <- lm(pov ~., data = training)
summary(model3)
```

```{r}
fit <- model3$fitted.values
true <- training$pov
summary_MLR3_train <- eval_results(true, fit)
```

We then evaluate the model between train and test
```{r}
fit <- predict(model3, newdata=test)
true <- test$pov
summary_MLR3_test <- eval_results(true, fit)
```

We can see the evaluation of the model from the function below. From the train model, the R^squared value will be 0.965 and for the test model it will be 0.881.

```{r}
summary_MLR <- rbind(summary_MLR3_train[-5], 
                     summary_MLR3_test[-5])
rownames(summary_MLR) <- c("Baseline MLR_Train", 
                           "Baseline MLR_Test" )
knitr::kable(summary_MLR, digits = 3)
```
Next we will be performing ridge and lasso regression model using the following code. We will split the data to train and test.
```{r}
# Training Data
data <- training
train.x <- data.matrix(data[,-ncol(data)])
train.y <- data.matrix(data[,ncol(data)])

# Test Data
data <- test
test.x <- data.matrix(data[,-ncol(data)])
test.y <- data.matrix(data[,ncol(data)])
```

Next we will perform the Ridge regression model.
```{r}
# Fit Ridge Regression Model
set.seed(123)
cv_ridge <- cv.glmnet(train.x, train.y, alpha = 0)
glm_ridge <- glmnet(train.x, train.y, 
                    alpha = 0, lambda = cv_ridge$lambda.min)

# Evaluate Ridge Regression Model on Training Set
fit <- predict(glm_ridge, newx = train.x, type = "response")
true <- train.y
summary_ridge_train <- eval_results(true, fit)


# Evaluate Ridge Regression Model on Test Set
fit <- predict(glm_ridge, newx = test.x, type = "response")
true <- test.y
summary_ridge_test <- eval_results(true, fit)

# Summarise Ridge Regression Model Performance into Table
summary_ridge <- rbind(summary_ridge_train, summary_ridge_test)
rownames(summary_ridge) <- c("Ridge Model_Train", "Ridge Model_Test" )
```

Next we will perform Lasso Regression model

```{r}
# Fit LASSO Model
set.seed(123)
cv_lasso <- cv.glmnet(train.x,train.y, alpha = 1)
glm_lasso <- glmnet(train.x,train.y, alpha = 1, 
                    lambda = cv_lasso$lambda.min)

# Evaluate LASSO Model on Training Set
fit <- predict(glm_lasso, newx = train.x)
true <- train.y
summary_lasso_train <- eval_results(true, fit)

# Evaluate LASSO Model on Test Set
fit <- predict(glm_lasso, newx = test.x)
true <- test.y
summary_lasso_test <- eval_results(true, fit)

# Summarise LASSO Model Performance into Table
summary_lasso <- rbind(summary_lasso_train, summary_lasso_test)
rownames(summary_lasso) <- c("LASSO Model_Train", "LASSO Model_Test" )
```

We will compare the 3 models that we have build, Multi linear ,Ridge and Lasso regression models. We evaluate the 3 models together and compare both train and test models of each of the 3 regression model.

```{r}
summary_3models <- rbind(summary_MLR3_train, 
                         summary_ridge_train,                               
                         summary_lasso_train, 
                         summary_MLR3_test,                             
                         summary_ridge_test, 
                         summary_lasso_test)

rownames(summary_3models) <- c("Baseline MLR_Train", "Ridge Model_Train", 
                               "LASSO Model_Train",  "Baseline MLR_Test",  
                               "Ridge Model_Test", "LASSO Model_Test" )

knitr::kable(summary_3models[1:3,-5], digits = 3)
knitr::kable(summary_3models[4:6,-5], digits = 3)
```
From the 3 models we can see that the original multi linear regression model is the best one with the highest R^squared value. However the Lasso Regression Model has the lowest MSE, MAE and RMSE. We can conclude that the reason for the first model having the highest R^squared value might be due to overfitting as many values will get removed. Next we will build other models to see if they work better.

## 3.2. Selectively remove variables with high missing rate

```{r}
needs("knitr","readr","tidyr","dplyr","ggplot2","stats","tidyverse","e1071","car","glmnet","corrplot","plotmo")
```

```{r}
countries <- read.csv("../data/countries.csv")
```

###  3.2.1. Model Fitting
#### A. Ordinary Linear Regression
Previous analysis showed that of all the variables in the countries.csv dataset, many of them had a high missing rate of above 35%, an arbitrary percentage that our group has chosen to filter the predictor variables. This section attempts to build a model on variables that have missing rates of below 35%.

The previous analysis showed that the relations between the predicted variable and gdp.pc, gdp.dflt, and fdi are better fitted to logarithmic models. Hence, we transformed the data with the mutate() function before building the machine learning model.
```{r}
subset0 <- subset(countries, select = c("country.code", "year","edu.total", "fdi","gcf","gdp.dflt","hlth","income","lbr.part","mil","pop.gwth.rural","pop.gwth.urban","pov","trade", "unemp", "year","gdp.pc" ))

subset1_0 <- subset0 %>% mutate(lfdi = log(subset0$fdi)) %>% mutate(lgdp.pc = log(subset0$gdp.pc)) %>% mutate(lgdp.dflt = log(subset0$gdp.dflt))
```
subset1 contains the variables below 35% of missing values. 
```{r}
subset1 <- subset(subset1_0, select = c("edu.total", "lfdi","gcf","lgdp.dflt","hlth","income","lbr.part","mil","pop.gwth.rural","pop.gwth.urban","pov","trade", "unemp", "year","lgdp.pc"))

subset1 <- na.omit(subset1)
```
We also performed standardisation to reduce the variance of our machine learning models. Below are the functions used to aid our standardisation step. 
```{r}
sd0 <- function(vct) {
  if (!is.numeric(vct)) {
    return(NA)
  }
  
  return(sd(vct, na.rm = T))
}


std0 <- function(vct, scl) {
  if (!is.numeric(vct)) {
    return(vct)
  }
  
  return(vct/scl)
}

scale <- function(data) {
  unlist(lapply(data, sd0))
}

standardise <- function(data) {
  scaler <- scale(data)
  
  numCols <- which(unlist(lapply(data, is.numeric)))
  num <- as.data.frame(mapply(std0, vct = data[,numCols], scl = scaler[numCols], SIMPLIFY = T))
  fct <- data[,-numCols]
  return(list(cbind(fct, num), scaler))
}


```

```{r}
df.countries0 <- standardise(subset1)[[1]]
scaler2 <- standardise(subset1)[[2]]
df.countries <- df.countries0
```

```{r}
#Divide each feature/target by its standard deviation
head(df.countries)
```

First, we split the data into training and testing data. 
```{r}
set.seed(123)
index <- sort(sample(x = nrow(df.countries), size = nrow(df.countries) * 0.8))
train_1 <- subset1[index,]
test_1 <- subset1[-index,]
```

Our first attempt 
```{r}
#Everything
lm_1 <- lm(data= train_1, formula = pov ~ .)
summary(lm_1)
```

The summary of `lm_1` suggests that the model still contains predictor variables with pr(>|t|) > 0.05. We can imply that these variables do not provide a high significance to our model and are thus able to be removed. To avoid removing significant variables, we shall conduct the removal of the variables deliberately. In `lm_1`, we notice that the variable `lgdp.dflt` has a high Pr(>|t|). Hence, we remove `lgdp.dflt`.

```{r}
#remove hlth 
lm_2 <- lm(data= train_1, formula = pov ~ .-lfdi - lgdp.dflt)
summary(lm_2)
plot(lm_2, which = 5)
```

```{r}
#Remove lgdp.dflt
lm_3 <- lm(data= train_1, formula = pov ~ . -lgdp.dflt)
summary(lm_3)
plot(lm_3, which = 5)
```

```{r}
#remove lfdi
lm_4 <- lm(data= train_1, formula = pov ~ . -hlth - lgdp.dflt - lfdi)
summary(lm_4)
plot(lm_4, which = 5)
```
Our model shows that among all the 14 predictor variables that we started with, only 11 of them showed significance in building the model. 

We then evaluated the results of the model. 
```{r}
#evaluating lm_4
eval.metrics.linreg <- function(actual, predicted) {
  residual <- actual - predicted
  mse <- mean(residual ^ 2)
  mae <- mean(abs(residual))
  rmse <-  sqrt(mse)
  mape <- mean(abs(residual / actual)) * 100
  
  
  data.frame(
    MSE = mse,
    MAE = mae,
    RMSE = rmse,
    MAPE = mape
  )
}
```

```{r}
actual <- train_1$pov
predicted <- predict(lm_4, newdata = train_1)
eval.metrics.linreg(actual, predicted)
```
The MAPE shows `Inf` due to the presence of 0 values in the Y variable. 

#### B. Regularization
Manual multiple regression models are prone to standard error. Hence, Ridge regression and LASSO regression are also used to build our model. 

Next, we will perform Ridge regression and LASSO regression in a dataset that contains `country.code` and `year` in addition to the previous dataset that we used for the previous linear regression model. 
```{r}
#Subsetting the data

data0 <- subset(subset1_0, select = c("country.code","year","edu.total", "lfdi","gcf","lgdp.dflt","hlth","income","lbr.part","mil","pop.gwth.rural","pop.gwth.urban","pov","trade", "unemp", "year","lgdp.pc"))
data <- na.omit(data0)
train.x <- data.matrix(subset(data, select = -pov))
train.y <- data.matrix(data$pov)
```

Splitting the aforementioned dataset into training and testing data. 

```{r}
split_train_test <- function(data) {
  set.seed(1984)
  # split data
  seeds <- data %>% 
    group_by(country.code) %>% 
    filter(row_number() == 1)
  
  plants <- data %>% 
    group_by(country.code) %>% 
    filter(row_number() != 1)
  
  isComplete <- which(complete.cases(plants))
  idx <- sample(isComplete, replace = F, 0.2 * nrow(plants))
  
  training <- plants[-idx,] %>% 
    rbind(seeds)
  test <- plants[idx,]
  
  return(list(training, test))
}
```

```{r}
training <- split_train_test(data)[[1]]
test <- split_train_test(data)[[2]]
```
Further splitting the training and testing data into  `train.x` and `train.y`
```{r}
ntrain <- 1:nrow(training)
combine <- rbind(training, test)
combine.mtrx <- model.matrix(pov ~ ., data = combine)
train.mtrx <- combine.mtrx[ntrain,]
test.mtrx <- combine.mtrx[-ntrain,]

train.x <- train.mtrx[,-1]
train.y <- training$pov

test.x <- test.mtrx[,-1]
test.y <- test$pov
```

Building a model with the lambda that outputs the smallest MSE. 
```{r}
cv_ridge <- cv.glmnet(train.x, train.y, alpha = 0)
glm_Ridge <- glmnet(train.x, train.y, alpha = 0, lambda = cv_ridge$lambda.min)

t(coef(glm_Ridge))
```
The ridge regression model shows that some countries have a higher influence on the model than others. At first glance,it seems like the Y variable, `pov`, depends heavily on whether or not it belongs to certain countries.To further illustrate the significance of `country.code`, we will plot the ridge regression graph for different lambda values. 
```{r}
#generate sequence of lambda values 
lambda <- 10^seq(-6, 2, length = 100)
ridge_model = glmnet(train.x, train.y, alpha = 0, lambda = lambda)

head(t(coef(ridge_model)))
```
Plotting the coefficients against different lambda values. 

```{r}
plot_glmnet(ridge_model)
```

Interesting! For a low lambda value, the model exhibits `cntr.MDG`, which represents Madagascar, followed by `cntr.ZMB`, which represents Zimbabwe,  as key predictor variables.
This means that the model depends highly on whether the country is Madagascar or Zimbabwe, followed by some other significant countries. The other variables that previously defined the model is now undermined by the presence of the `country.code` variable and does not show high influence. 

We then proceeded to evaluate the results 

```{r}
eval_results <- function(fit, true) {
  actual <- data.matrix(true)
  SSE <- sum((actual - fit)^2)
  SST <- sum((actual - mean(actual))^2)
  R_square <- 1 - SSE/SST
  data.frame(
    MSE = MSE(fit, true),
    MAE = MAE(fit, true),
    RMSE = RMSE(fit, true),
    MAPE = MAPE(fit, true),
    R2  = R_square
  )
}
```

```{r}
fit <- predict(glm_Ridge, train.x)
true <- train.y
summary_Ridge_train <- eval_results(fit, true)
summary_Ridge_train
```
The training dataset produced a much higher value compared to the regular multiple regression model shown above. 

Fitting the model to the test dataset,

```{r}
fit <- predict(glm_Ridge, test.x)
true <- test.y
summary_Ridge_test <- eval_results(fit, true)
summary_Ridge_test
```
A slightly lower R^2 value, but still proves that the model is good enough. 

Below is the summary of the results of the training and testing dataset. 
```{r}
summary_Ridge <- rbind(summary_Ridge_train, summary_Ridge_test)
rownames(summary_Ridge) <- c("Ridge_train", "Ridge_test")
knitr::kable(summary_Ridge, digits = 3)
```
Next, we attempted LASSO regression as well. The steps are essentially similar to that of Ridge regression. 
```{r}
#Train a LASSO regression model
set.seed(123)
cv_lasso <- cv.glmnet(train.x, train.y, alpha = 1, type.measure = "mse")
```

```{r}
cv_lasso
```

```{r}
plot(cv_lasso)
```

```{r}
glm_Lasso <- glmnet(train.x, train.y, alpha = 1, lambda = cv_lasso$lambda.min)
t(coef(glm_Lasso))
```
It is evident that the number of variables included in the model is less than that of ridge regression. This is in accordance with the nature of the LASSO regression model that reduces the coefficients of certain predictor variables to zero for model interpretability. 

```{r}
lambda <- 10^seq(-6, 2, length = 100)
lasso_model = glmnet(train.x, train.y, alpha = 1, lambda = lambda)

t(coef(lasso_model))
```
```{r}
plot_glmnet(lasso_model)
```
```{r}
plot_glmnet(ridge_model)
```

```{r}
fit2 <- predict(glm_Lasso, train.x)
true2 <- train.y
summary_Lasso_train <- eval_results(fit2, true2)
summary_Lasso_train
```

```{r}
plot_glmnet(lasso_model)
```

```{r}
fit3 <- predict(glm_Lasso, test.x)
true3 <- test.y
summary_Lasso_test <- eval_results(fit3, true3)
summary_Lasso_test
```
```{r}
summary_lasso <- rbind(summary_Lasso_train, summary_Lasso_test)
rownames(summary_lasso) <- c("LASSO_train", "LASSO_test")
knitr::kable(summary_lasso, digits = 3)

```
###  3.2.2. Assessment

This section recapitulates the three models that we have generated so far through the method of gradually removing variables from the original dataset. 

```{r}
summary(lm_4)
```

Our first model, `lm_4`, generated with regular multiple linear regression exhibits the lowest R^2 value. The model shows that `incomeL` is the most significant predictor variable with the highest coefficient. One interesting observation is that `incomeLM` and `incomeUM` has a negative correlation with the pov level, in contrast to `incomeL`. This shows that countries that have lower income tend to have high `pov` value. On the other hand, countries that are classified to have lower middle income and upper middle income seem to have lower `pov` values, as shown by their negative coefficients in the regression model.

Apart from income, some other variables that contribute more significantly to the model are `lgdp.pc`, with a coefficient of -3.13114, exhibiting a stronger negative correlation, and `pop.gwth.urban`, exhibiting a stronger positive correlation with a coefficient of 1.90680. This is counter-intuitive, as the notion is that population growth in cities will help drive positive economic change due to the number of productive people living in the city. 
The model also shows that `pop.gwth.rural` gives a less significant impact on `pov` as compared to  `pop.gwth.rural`. 


```{r}
knitr::kable(summary_lasso, digits = 3)
```

```{r}
knitr::kable(summary_Ridge, digits = 3)
```
The MSE for the LASSO regression model is slightly higher than that of the Ridge regression model. This to be expected, as the Ridge regression model aims for an improved accuracy while the LASSO model aims for an improved interpretability. Both models seem to exhibit very high R^2 value for both the training and testing dataset. 

```{r}
plot_glmnet(lasso_model)
```

```{r}
plot_glmnet(ridge_model)
```
```{r}
plot(lm_4, which = 5)
```
The `Residual vs Leverage` plot of `lm_4` suggests that heteroskedasticity might be present in the model. This means that the variance of the residuals is unequal over a range of observed values. This problem exists in the dataset rather than the models generated.


## 3.3. Forward variable selection
###  3.3.1. Model Fitting 
```{r}
df_count<-read.csv("../data/countries1.csv")
set.seed(10)
index <- sort(sample(x = nrow(df_count), size = nrow(df_count)*.8))
train <- df_count[index, ]
test <- df_count[-index, ]
```

Through forward selection, we will do feature selection. We will stop the forward selection when the adjusted r-value>0.75. 

Through iteration of different models. A linear model consisting of income as a predictor has the highest adjusted r-square value=0.6201, thus it is selected. 

```{r cars}
summary(lm(formula = pov ~ income,data = train)) #0.6201
```

Since income as a predictor brought the highest adjusted r-value, we add another predictor to income to find the predictor added to income which produced the highest adjusted r-value. reg as a predictor has the highest adjusted r-value=0.7181

```{r}
summary(lm(formula = pov ~ reg+income,data = train)) #0.7181
```

The forward selection process continues and lgdp.pc is the new predictor added that produced highest adjusted r-square=0.7492
```{r}
summary(lm(formula = pov ~ lgdp.pc+income+reg,data = train)) #0.7492
```

The addition of predictor gdp.pc produces the highest adjusted r-square value above 0.75, adjusted r-square=0.7703
```{r}
summary(lm(formula = pov ~ gdp.pc+income+reg+lgdp.pc,data = train)) #0.7703
```

Fitting of multiple linear regression with predictor gdp.pc, income, reg and lgd.pc.
```{r}
lm1<-lm(formula = pov ~ gdp.pc+income+reg+lgdp.pc,data = train)
summary(lm1)
```

###  3.3.2. Assessment
Assessment of linear model lm1: 
the predicted model has some NA values which are removed. The error rates are too high, therefore, this method of variable selection may not be suitable
```{r}
eval.metrics.linreg <- function(actual, predicted) {
  residual <- actual - predicted
  mse <- mean(residual ^ 2)
  mae <- mean(abs(residual))
  rmse <-  sqrt(mse)
  mape <- mean(abs(residual / actual)) * 100
  
  data.frame(
    MSE = mse,
    MAE = mae,
    RMSE = rmse,
    MAPE = mape
  )
}
predicted<-predict(lm1,newdata=test)
actual<-test$pov
df23<-na.omit(data.frame(actual,predicted))
ac<-df23$actual
pr<-df23$predicted
eval.metrics.linreg(ac,pr)
```

###  3.3.3. Interpretation
gdp per capital, income class and region of the country can provide a model to predict the poverty headcount ratio in a country. The error rate of the model is high on the testing data set, showing it may not be a good model.

## 3.4. Imputation
**What is imputation?**  
Imputation is a technique to handle missing values by replacing missing data with substitute values. In our study, we focus on "item imputation", which means substituting for a component of a data point (i.e. a variable). The general idea is to take a value that preserve the property of the data (e.g., distribution, mean, standard deviation), and use other variables to predict the missing values (much like regression). Since we observed some correlations during descriptive analysis, we expect the imputation algorithm to work well.

We use **Multivariate Imputation By Chained Equations (MICE)** as primary tool for this technique, because of its wide acceptance in scientific studies (@imputWorks). In order to use this technique, we have to assume that the "missingness" of a field can be explained by the values in other columns, (e.g., If the countries is in North America, it's more likely to be missing as we have discovered in 2.2). The general ideas of the algorithm is to fill in the missing values, and improve it iteratively until predicted values converse to a stable point. The algorithmic details of MICE is very concisely (and enthrallingly) explained in @micealgo.  

We use relatively small parameters in the interest of time.

- `m = 3` imputed data sets
- `maxit = 20` iterations

The imputation method is `cart` (Classification and Regression Trees).

```{r}
set.seed(1984)
# split data, make sure all the coutries
seeds <- countries1 %>% 
  group_by(country.code) %>% 
  filter(row_number() == 1)

plants <- countries1 %>% 
  group_by(country.code) %>% 
  filter(row_number() != 1)

isComplete <- which(complete.cases(plants))
idx <- sample(isComplete, replace = F, 0.2 * nrow(plants))

countries.train.4 <- plants[-idx,] %>% 
  rbind(seeds)
countries.test.4 <- plants[idx,]

meth <- c(rep("",4), "cart", "", rep("cart", 16))
names(meth) <- colnames(countries1)
meth

# run the algorithm, or read from the file we generated as this is a very time-consuming process
# countries1.imputed <- mice(countries.train.4, m = 3, maxit = 20, method = meth)
# saveRDS(countries1.imputed, 'countries1.imputed.RData')
countries1.imputed <- readRDS('countries1.imputed.RData')
summary(countries1.imputed)
```
We can take a look into one of the imputed data sets.
```{r}
countries1.imputed.1 <- complete(countries1.imputed, 1)
summary(countries1.imputed.1)
sum(!complete.cases(countries1.imputed.1))
```
We found no missing cases as expected.

### 3.4.1. Model Fitting
#### A. Ordinary Linear Regression
We generated 3 sets of imputed (training) data. We can build separate models using each data set, and combine the estimates using [pooling rule](https://amices.org/mice/reference/pool.html#details). With all the generated data sets.

```{r}
formula.str <- 'pov ~ country.code+year+income+edu.total+hlth+mil+fdi+lbr.part+unemp+pop.gwth.total+pop.gwth.rural+pop.gwth.urban+gdp.dflt+gcf+trade+gdp.pc+lfdi+lgdp.dflt+lgdp.pc'

# Using single imputed data set
fit1 <- lapply(1:countries1.imputed$m, \(i) lm(as.formula(formula.str), data = complete(countries1.imputed, i)))

# Build models with all generated data set and pool the estimates
fit2 <- with(data = countries1.imputed, exp = lm(as.formula(formula.str)))
fit2.combined <- pool(fit2)
# dummy lm model
fit2.dummy <- lm(as.formula(formula.str), data = complete(countries1.imputed, 1))
# replace coefficients of dummy model
name.coef <- names(fit2.dummy$coefficients)
fit2.dummy$coefficients <- fit2.combined$pooled$estimate
names(fit2.dummy$coefficients) <- name.coef
```
Helper function to calculate R-Squared
```{r}
r2 <- function(pred, orig) {
   RSS <- sum((pred - orig)^2)
   TSS <- sum((orig - mean(orig))^2)
   R2 <- 1 - RSS/TSS
   return (R2)
}

adjR2 <- function(pred, orig, k) {
  R2 <- r2(pred, orig)
  n <- length(pred)
  adjr2 <- 1 - (1-R2)*(n-1)/(n-k-1)
  return (adjr2)
}
```
R-Squared on train and test
```{r}
fit1.summ <- lapply(fit1, \(f) {
  # number of variables
  k <- length(f$coefficients) - 1
  # predict value of test
  pred <- predict(f, countries.test.4)
  test.resid <- pred - countries.test.4$pov
  data.frame(
    `Train MSE` = mean(summary(f)$residuals^2),
    `Test MSE` = mean(test.resid^2),
    `Train MAE` = mean(abs(summary(f)$residuals)),
    `Test MAE` = mean(abs(test.resid)),
    `Train R2` = summary(f)$r.squared,
    `Test R2` = r2(pred, countries.test.4$pov)
  )
})

# number of variables
k <- length(fit2.dummy$coefficients) - 1
# number of variables
fit2.pred <- predict(fit2.dummy, countries.test.4)
test.resid <- fit2.pred - countries.test.4$pov
fit2.summ <- data.frame(
  `Train MSE` = mean(summary(fit2.dummy)$residuals^2),
  `Test MSE` = mean(test.resid^2),
  `Train MAE` = mean(abs(summary(fit2.dummy)$residuals)),
  `Test MAE` = mean(abs(test.resid)),
  `Train R2` = pool.r.squared(fit2)[,'est'],
  `Test R2` = r2(fit2.pred, countries.test.4$pov)
)

res <- do.call(rbind.data.frame, fit1.summ)
res <- rbind(res, fit2.summ)
res <- cbind(data.frame(Set = c(1:3, "Pooled")), res)

kable(res)
```

Pooled model slightly improve performance. Test data fitting has better MSE and MAE than train data. However,  R-Squared is lower in test data. There might be some over-fitting in our models.

```{r}
# find coef of variable other than country.code
var <- c("year","incomeL","incomeLM","incomeUM","edu.total","hlth","mil","fdi","lbr.part","unemp","pop.gwth.total","pop.gwth.rural","pop.gwth.urban","gdp.dflt","gcf","trade","gdp.pc","lfdi","lgdp.dflt","lgdp.pc")

res <- do.call(rbind, lapply(fit1, \(f) f$coefficients[var]))
res <- rbind(res, fit2.dummy$coefficients[var])
res <- cbind(data.frame(Set = c(1:3, "Pooled")), res)
kable(res)
```

In contradiction to expectation, `hlth` is positively correlated with `pov`, and lower-middle income countries (incomeLM) are less poor than high income countries (incomeH). `pop.gwth.total` is negatively related to `pov`, while `pop.gwth.rural` and `pop.gwth.urban` are positively related. They might be indications of over-fitting model.

```{r}
# fit model on imputed set no. 1
summary(fit1[[1]])
```

There are also some weak variable, we should perform some variable selection.

#### B. VIF

Our data set contains a lots of variables. We can perform some variable selection to reduce over-fitting.

```{r}
gvif <- lapply(fit1, vif)
gvif
```

[Some forum](https://stats.stackexchange.com/questions/70679/which-variance-inflation-factor-should-i-be-using-textgvif-or-textgvif) suggested that we should use the standard $GVIF^{1/(2 \cdot df)} < 2$ as equivalent to $GVIF < 4$ to account for high degree of freedom of some variables.

```{r}
# adjusted gvif higher than 3
lapply(gvif, \(g) {
  g[g[,3] > 3, 3]
})
```

Remove the highest-ranked variables (`pop.gwth.total`, `pop.gwth.urban`, `pop.gwth.rural`, `gdp.pc`, `lgdp.pc`) and rebuild the models.

```{r}
formula.str <- 'pov ~ country.code+year+income+edu.total+hlth+mil+fdi+lbr.part+unemp+gdp.dflt+gcf+trade+lfdi+lgdp.dflt'

# Using single imputed data set
fit1 <- lapply(1:countries1.imputed$m, \(i) lm(as.formula(formula.str), data = complete(countries1.imputed, i)))
```

Re-run GVIF analysis

```{r}
gvif <- lapply(fit1, vif)

lapply(gvif, \(g) {
  g[g[,3] > 2, 3]
})
```

There're still some variable with adjusted GVIF > 2. We are interested in the effects of `hlth` (expenditure in Healthcare), so we won't remove those variables. We can remove `trade`.

```{r}
# remove trade
formula.str <- 'pov ~ country.code+year+income+edu.total+hlth+mil+fdi+lbr.part+unemp+gdp.dflt+gcf+lfdi+lgdp.dflt'

# Using single imputed data set
fit1 <- lapply(1:countries1.imputed$m, \(i) lm(as.formula(formula.str), data = complete(countries1.imputed, i)))

# gvif
gvif <- lapply(fit1, vif)

lapply(gvif, \(g) {
  g[g[,3] > 2, 3]
})
```

The adjusted GVIF are acceptably low.

Coefficients overview.

```{r}
# find coef of variable other than country.code
var <- c("year","incomeL","incomeLM","incomeUM","edu.total","hlth","mil","fdi","lbr.part","unemp","gdp.dflt","gcf","lfdi","lgdp.dflt")

res <- do.call(rbind, lapply(fit1, \(f) f$coefficients[var]))
kable(res)
```

There are several sign-switching in `income` coefficients, indicating the effect of removing some multicollinearity.

```{r}
fit1.summ <- lapply(fit1, \(f) {
  k <- length(f$coefficients) - 1
  pred <- predict(f, countries.test.4)
  data.frame(
    `Train Adj.R2` = summary(f)$r.squared,
    `Test Adj.R2` = r2(pred, countries.test.4$pov)
  )
})

res <- do.call(rbind.data.frame, fit1.summ)
res <- cbind(data.frame(Set = 1:3), res)
kable(res)
```

Directly removing variables seems to be penalising our test results. As our interest is to investigate the effect of certain variables on poverty, the yielded R-Squared is within acceptable range. We can continue variable selection on the new models.

```{r}
formula.str <- 'pov ~ country.code+year+income+edu.total+hlth+mil+fdi+lbr.part+unemp+gdp.dflt+gcf+lfdi+lgdp.dflt'
```


#### B. Step-wise AIC
We can conduct a step-wise AIC variable selection. It is similar to the procedure we use in class, but based on a metric call AIC (Akaike Information Criterion), which is an estimator of prediction error and relative quality of statistical models. The lower AIC is, the better the model fits. \\

```{r}
# helper
fitAIC <- function(i) {
  set.seed(1984)
  fit <- lm(as.formula(formula.str), complete(countries1.imputed, i))
  aic.fit <- stepAIC(fit, trace = F, direction = 'backward')
  return(aic.fit)
}

# build models
aic.fits <- lapply(1:countries1.imputed$m, fitAIC)

# check final terms of each model
lapply(aic.fits, \(mod) formula(mod$terms))
```

The most commonly removed variables are:

- `gdp.dflt`
- `ldgp.dflt`
- `unemp`
- `fdi`

```{r}
# predict with each model
aic.pred <- lapply(aic.fits, predict, newdata = countries.test.4)
# calculate adj r2
aic.train.r2 <- unlist(lapply(aic.fits, \(model) summary(model)$adj.r.squared))

k <- lapply(aic.fits, \(m) length(m$coefficients))
aic.test.r2 <- unlist(mapply(r2, aic.pred, MoreArgs = list(orig = countries.test.4$pov)))

res <- data.frame(`set no.` = 1:3, train = aic.train.r2, test = aic.test.r2)
```

We can compare R-Squared to estimate relative over-fitting in the new models.

```{r}
kable(res)
```

Performance on train set and test set are slightly improved. However, we have simplified the model by removing some unnecessary terms. We can update our models.

```{r}
formula.str <- 'pov ~ country.code+year+income+edu.total+hlth+mil+lbr.part+gcf+lfdi'

fit3 <- lapply(1:countries1.imputed$m, \(i) lm(as.formula(formula.str), data = complete(countries1.imputed, i)))

# Build models with all generated data set and pool the estimates
fit4 <- with(data = countries1.imputed, exp = lm(as.formula(formula.str)))
fit4.combined <- pool(fit4)
# dummy lm model
fit4.dummy <- lm(as.formula(formula.str), data = complete(countries1.imputed, 1))
# replace coefficients of dummy model & predict
fit4.dummy$coefficients <- fit4.combined$pooled$estimate

fit3.summ <- lapply(fit3, \(f) {
  k <- length(f$coefficients) - 1
  pred <- predict(f, countries.test.4)
  test.resid <- pred - countries.test.4$pov
  data.frame(
    `Train MSE` = mean(summary(f)$residuals^2),
    `Test MSE` = mean(test.resid^2),
    `Train MAE` = mean(abs(summary(f)$residuals)),
    `Test MAE` = mean(abs(test.resid)),
    `Train R2` = summary(f)$r.squared,
    `Test R2` = r2(pred, countries.test.4$pov)
  )
})

k <- length(fit4.dummy$coefficients) - 1
fit4.pred <- predict(fit4.dummy, countries.test.4)
test.resid <- fit4.pred - countries.test.4$pov
fit4.summ <- data.frame(
  `Train MSE` = mean(summary(fit4.dummy)$residuals^2),
  `Test MSE` = mean(test.resid^2),
  `Train MAE` = mean(abs(summary(fit4.dummy)$residuals)),
  `Test MAE` = mean(abs(test.resid)),
  `Train R2` = pool.r.squared(fit4)[,'est'],
  `Test R2` = r2(fit4.pred, countries.test.4$pov)
)

res <- do.call(rbind.data.frame, fit3.summ)
res <- rbind(res, fit4.summ)
res <- cbind(data.frame(Set = c(1:3, "Pooled")), res)

kable(res)
```

Coefficients overview.

```{r}
var <- c("year","incomeL","incomeLM","incomeUM","edu.total","hlth","mil","lbr.part","gcf","lfdi")

res <- do.call(rbind, lapply(fit3, \(f) f$coefficients[var]))
kable(res)
```

We can further treat over-fitting with regularization.

#### C. Regularization

Helper functions to standardise data.

```{r}
sd0 <- function(vct) {
  if (!is.numeric(vct)) {
    return(NA)
  }
  
  return(sd(vct, na.rm = T))
}


std0 <- function(vct, scl) {
  if (!is.numeric(vct)) {
    return(vct)
  }
  
  return(vct/scl)
}

standardise <- function(train, test) {
  # save number of train and combine both data set
  trainCases <- 1:nrow(train)
  data <- rbind(train, test)
  # calc scaler
  scaler <- unlist(lapply(data, sd0))
  # get the numeric columns
  numCols <- which(unlist(lapply(data, is.numeric)))
  # divide numeric columns by scalers, and combine with factor columns
  num <- as.data.frame(mapply(std0, vct = data[,numCols], scl = scaler[numCols], SIMPLIFY = T))
  fct <- data[,-numCols]
  final <- cbind(fct, num)
  # split to train and test
  trainSet <- final[trainCases,]
  testSet <- final[-trainCases,]
  
  res <- list(final = final, train = trainSet, test = testSet, scaler = scaler)
  return(res)
}
```

Helper function to build the optimal model.

```{r}
optimalModel <- function(formula, train, test) {
  set.seed(1984)
  # standise and save number of train cases (to split later)
  std.data <- standardise(train, test)
  trainCases <- 1:nrow(train)
  # matrix-ise
  xs <- model.matrix(formula, std.data$final)[,-1]
  ys <- std.data$final$pov
  # split data
  train.x <- xs[trainCases,]
  train.y <- ys[trainCases]
  test.x <- xs[-trainCases,]
  test.y <- ys[-trainCases]
  # list of tested alpha, resolution = 0.1
  alphas <- seq(0, 1, 0.1)
  # build a bunch of models
  models <- lapply(alphas, \(a) cv.glmnet(train.x, train.y, type.measure = "mse", alpha = a))
  cv.error <- unlist(lapply(models, \(model) model$cvm[model$lambda == model$lambda.min]))
  # best model
  best.model.idx <- which.min(cv.error)
  # optimal alpha and lambda
  alpha.opt <- alphas[best.model.idx]
  lambda.opt <- models[[best.model.idx]]$lambda.min
  best.model <- glmnet(train.x, train.y, alpha = alpha.opt, lambda = lambda.opt)
  # predict for test data
  train.fit <- predict(best.model, train.x)
  test.fit <- predict(best.model, test.x)
  # evaluation - train
  k <- best.model$df
  train.r2 <- best.model$dev.ratio
  train.adjR2 <- adjR2(train.fit, train.y, k)
  train.resid <- train.fit - train.y
  train.mse <- mean(train.resid^2)
  train.rmse <- sqrt(train.mse)
  train.mae <- mean(abs(train.resid))
  train.mape <- mean(abs(train.resid / train.y))
  # evaluation - test
  test.r2 <- r2(test.fit, test.y)
  test.adjR2 <- adjR2(test.fit, test.y, k)
  test.resid <- test.fit - test.y
  test.mse <- mean(test.resid^2)
  test.rmse <- sqrt(test.mse)
  test.mae <- mean(abs(test.resid))
  test.mape <- mean(abs(test.resid / test.y))
  # final results
  results <- list(
    train = list(
      data = train,
      r2 = train.r2,
      adj.r2 = train.adjR2,
      fitted = train.fit,
      residuals = train.resid,
      mse  = train.mse,
      rmse = train.rmse,
      mae  = train.mae,
      mape = train.mape
    ),
    test = list(
      data = test,
      r2 = test.r2,
      adj.r2 = test.adjR2,
      fitted = test.fit,
      residuals = test.resid,
      mse  = test.mse,
      rmse = test.rmse,
      mae  = test.mae,
      mape = test.mape
    ),
    scaler = std.data$scaler,
    alpha = alpha.opt,
    lambda = lambda.opt,
    model = best.model
  )
  
  return(results)
}
```

Build the optimal models using each imputed data set.

```{r}
# helper function to build from a specific imputed data
buildWith <- function(set, imputed, test, formula, fun) {
  train <- complete(imputed, set)
  model <- fun(formula, train, test)
  return(model)
}

set.seed(1984)

models <- lapply(1:countries1.imputed$m, buildWith, imputed = countries1.imputed, test = countries.test.4, formula = as.formula(formula.str), fun = optimalModel)
```

Display parameters and evaluation.

```{r}
result.list <- lapply(models, \(mod) {
  data.frame(
    alpha = mod$alpha,
    lambda = mod$lambda,
    `Train MSE` = mod$train$mse,
    `Test MSE` = mod$test$mse,
    `Train MAE` = mod$train$mae,
    `Test MAE` = mod$test$mae,
    `Train R2` = mod$train$r2,
    `Test R2` = mod$test$r2
  )
})

result.df <- do.call(rbind.data.frame, result.list)
result.df <- cbind(data.frame(Set = 1:countries1.imputed$m), result.df)
kable(result.df)
```

There are some improvement on the test data performance based on adjusted R-Squared. MSE and MAE remain low on both data sets.

Coefficients overview.

```{r}
var <- c("year","incomeL","incomeLM","incomeUM","edu.total","hlth","mil","lbr.part","gcf","lfdi")

res <- do.call(rbind, lapply(models, \(f) coefficients(f$model)[var,]))
res <- cbind(data.frame(Set = 1:3), res)
kable(res)
```

All non-country.code variables are non-zero. We can observe that the regularisation models just remove the effect of `country.code`, for countries that have similar "baseline".

###  3.4.2. Assessment

Now we are evaluating some performance metrics, checking assumptions, and remedy some potential problems.

#### A. Homoscedasticity

Plot the residuals ~ pov

```{r}
for (mod in models) {
  resid <- mod$train$residuals
  fitted <- mod$train$fitted
  # plot(resid ~ pov, main = paste("Model", i))
  print(ggplot(mapping = aes(y = resid, x = fitted)) +
    geom_point() +
    geom_smooth(formula = y ~ x, method = loess))
}
```

There seems to be some larger variance with higher values of `pov`. We can conduct a statistical test to confirm the present of heteroscedasticity.

The Goldfeld-Quandt test is performed by eliminating a certain number of observations from the dataset’s center, then comparing the spread of residuals between the two datasets on either side of the central observations.

The Goldfeld-Quandt test examines two submodels’ variances divided by a defined breakpoint and rejects if the variances disagree.

Under H0, the Goldfeld-Quandt test’s test statistic follows an F distribution with degrees of freedom as specified in the parameter.

- **Null (H0):** Heteroscedasticity is not present.
- **Alternative (H1):** Heteroscedasticity is present.

```{r}
lapply(models, \(m) {
  resid <- m$train$residuals
  # apply Goldfeld-Quandt test
  gqtest(formula = resid ~ 1, order.by = m$train$fitted)
})
```

There is heteroscedasticity in our models. [Generalized Least Squares With Unknown For of Variance](https://rpubs.com/cyobero/187387) is a possible remedy of this problem, but at the cost of model interpretation. The high variance in higher end of `pov` can be explained by the relative volatile economical and political climate in highly poor countries (one standard deviation from mean), leading to unstable effect of predictors.

#### B. Independence

Some possible source of dependency are poverty measured on the same country, or in the same year. These are accounted in our models by controlling those variables. 

Some countries have been engaging in international wars, being under the influences of foreign forces such as Iraq, and Afghanistan. They possess similar (and different) political climates. Some other groups of economical alliance, or enjoying similar natural resources: OPEC, EU, ASEAN, etc. might have similar characteristics that make them dependent.

A solution to resolve this is to conduct the study on countries with similar geographical characteristics. In our study, however, we are interested in the general effects of certain variable across the globe. Although there are some dependence, the large number of entities with various features, we hypothesize, will cancel each other out, resulting in a net effect that gives an overview of the influences of variables.

#### C. Normally Distributed Residuals

```{r}
for (m in models) {
  p <- ggplot(mapping = aes(x = m$train$residuals)) +
    geom_histogram(aes(y = ..density..), bins = 30) +
    geom_density(alpha = .2, fill = "blue")
  
  print(p)
}
```

Although quite negatively skewed, the residuals are visibly normal.

#### D. Outliers

As elastic net models produced in the last section are not very fit for detecting outliers using R-support function. As they just removed dummy variables in `country.code`, we can using OLS to detect some potential outliers. They are the same models we produced after the step-AIC selection (`fit3` and `fit4`)

```{r}
for (mod in fit3) {
  plot(mod, which = 4)
}
```

Some obvious outliers are point 1112, 1329, 1395, 1396, 1424, 147, 1486, 1508.

```{r}
out <- c(1112, 1329, 1395, 1396, 1424, 147, 1486, 1508)
var <- c("country.code","country.name","year","pov","income","edu.total","hlth","mil","lbr.part","gcf","lfdi")
# See the original data instead of imputed data
countries.train.4[out,var]
```

These countries all appear very infrequently in our data set (only twice for South Sudan and St.Lucia). The gap between their appearance are quite large, too. Such reasons could have made the model struggle to predict their poverty states.

Yemen's data were collected in 1998, 2005, and 2014, a huge gap in time, by when it has doubled its poverty rate.

Guinea is an interesting point. Because the outlier data (87.9% in 1991) is quite different to the next data point (40.8% in 1994), a huge jump in just 3 years. This might be a consequence of the political changes (new constitution, establishment of the Supreme Court, etc.) in this period. They are the changes not captured by the variables.

Gambia has the issue with Guinea, when it saw a huge reduction of poverty of 25% from 1998-2003.

Lucia only has two data points with large gap in time (1995-2016). It can be difficult to know much about someone if you only met them twice, with 21 years in between.

These data seems valid and there is no convincing reasons to remove them from the model (in the case of Guinea and Gambia, maybe there was some false reports because the jump were quite suspicious, but we need more evidence to conclude that they are false data).

###  3.4.3. Interpretation

Final model coefficients (exclude `country.code`):

```{r}
var <- c("year","incomeL","incomeLM","incomeUM","edu.total","hlth","mil","lbr.part","gcf","lfdi")

res <- do.call(rbind, lapply(models, \(f) coefficients(f$model)[var,]))
res <- cbind(data.frame(Set = 1:3), res)
kable(res)
```

We should convert them to un-standardised value for easier interepretation.

```{r}
var <- c("year","incomeL","incomeLM","incomeUM","edu.total","hlth","mil","lbr.part","gcf","lfdi")

res <- do.call(rbind, lapply(models, \(f) {
  # get scaler saved in the model
  scaler <- f$scaler
  # categorical has scaler = 1
  scaler[c("incomeL","incomeLM","incomeUM")] <- 1
  # beta = std.beta * sigma_y / sigma_x
  coefficients(f$model)[var,] * scaler['pov'] / scaler[var]
}))
kable(res)
```

# 4. Conclusion

Approach 1, 2 and 4 seem to have the best performace.

First Approach:
```{r}
knitr::kable(summary_3models[1:3,-5], digits = 3)
knitr::kable(summary_3models[4:6,-5], digits = 3)
```

Second Approach:

```{r}
knitr::kable(summary_lasso, digits = 3)
knitr::kable(summary_Ridge, digits = 3)
```

Fourth Approach:
```{r}
kable(result.df[c('alpha', 'lambda', 'Train.R2', 'Test.R2')])
```

We can deduce our final interpretation from these models.

```{r}
var <- c('incomeL', 'incomeLM', 'incomeUM', 'edu.total', 'hlth', 'mil')
result1 <- coefficients(model3)[var]
# result1[names(scaler2)] <- result1[names(scaler2)] / scaler2[names(scaler2)]
# result1 <- result1 * scaler2['pov']
# result1 <- result1[var]

result2 <- coefficients(glm_Ridge)[,'s0']
result2[names(scaler2)] <- result2[names(scaler2)] / scaler2[names(scaler2)]
result2 <- result2 * scaler2['pov']
result2 <- result2[var]

result4 <- res[2,var]

# combine
finalResults <- data.frame(Approach = c(1,2,4))
finalResults <- finalResults %>% 
  cbind(rbind(result1, result2, result4))

kable(finalResults)
```

Some important interpretation:

- Low income countries generally have higher poverty index than High income countries.
- As suggested by 1st approach, Lower-Middle income countries have higher poverty index than High income countries. While 2nd and 4th approach suggest otherwise.
- Upper-Middle income countries in general have lower poverty index than High income countries.

As suggested by these models. Upper-Middle income countries have lowest poverty rate. This can be attributed to that these countries tend to have more even wealth distribution than high income countries, and have enough money to alleviate poor people.

- Spending on education is negatively correlated with poverty. On average, 1% (of GDP) more spending in education results in 3 point decrease in poverty index. This suggests that public spending on education can reduce poverty.
- Spending on Healthcare is negatively correlated with poverty. On average, 1% (of GDP) more spending in education results in 0.5 point decrease in poverty index.
- Spending on military has ambiguous effect on poverty. Different model results in different interpretation. This can be accounted to the difference in the data used in each approach. Approach 1 only use complete cases, which caused the model to predict positive relationship. However, as more data is utilised in approach 2&4 use more data, the effect tend to be neutral/negative.

# 5. References
