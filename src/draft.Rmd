---
title: "Draft Documents"
author: "Team 1 (Hedgehog, Callista, Imtiyaaz, Issac)"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document:
    df_print: paged
bibliography: citations.bib
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F)
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 50), tidy = TRUE)
```

```{r, message = F}
# Package names
# packages <- c("MASS", "knitr", "readr", "tidyr", "dplyr", "ggplot2", "e1071", "moments", "corrplot", "Hmisc", "PerformanceAnalytics", "mice", "needs")
# 
# # Install packages not yet installed
# installed_packages <- packages %in% rownames(installed.packages())
# if (any(installed_packages == FALSE)) {
#   install.packages(packages[!installed_packages])
# }
# 
# # Packages loading
# invisible(lapply(packages, library, character.only = TRUE))
# 
# prioritize(dplyr)

library(needs)

needs("dplyr", "MASS", "knitr", "readr", "tidyr", "ggplot2", "e1071", "moments", "corrplot", "Hmisc", "PerformanceAnalytics", "mice", "car", "glmnet")

prioritize(dplyr)
```

# 1. Introduction (To be done)
The project aim is to explore different models using the same dataset to evaluate which method will be the best to build a model for the dependent variable. The dataset we are using for this project has 1901 data values from 23 different variables. Our plan for this project is to first build a model with all the variables in the dataset. We will then follow it up by builidng a model without the missing values and build a ridge regression and Lasso regression model. Our second approach will be to selectively remove variables from the model and to build the best model  using regularisation and regression. Our next approach was to add the variables into the model one at a time to see if a different model can be achieved. Lastly, we performed imputation on our model to tackle the missing values present in our dataset.We then compared all of this approaches to come to a conclusion.

# 2. Data Characteristic

## 2.1. Nature of Data

The data set is collection The World Bank Data, the variables of interest are extracted from the raw data files and combined into a single data frame for analysis. The final data set includes:

1.  **country.code**: Country code\
2.  **country.name**: Country name\
3.  **year**: Year\
4.  **income**: Income class
-   Low income (L)\
-   Lower middle income (LM)\
-   Upper middle income (UM)\
-   High income (H)\
5.  **reg**: Region\
6.  **pov**: Poverty headcount ratio based on cut-off value of $2.15 per day\
7.  **mpi**: Multidimensional Poverty Index\
8.  **edu.total**: Total expenditure on education (% of GDP)\
9.  **edu.pri**: Total expenditure on primary education (% of total education expenditure)\
10. **edu.sec**: Total expenditure on secondary education (% of total education expenditure)\
11. **edu.ter**: Total expenditure on tertiary education (% of total education expenditure)\
12. **hlth**: Total expenditure on health (% of GDP)\
13. **mil**: Total expenditure on military (% of GDP)\
14. **fdi**: Foreign Direct Investment\
15. **lbr.part**: Labour force participation (% of population ages 15+)\
16. **unemp**: Unemployment rate\
17. **pop.gwth.total**: Total population growth rate\
18. **pop.gwth.rural**: Total rural population growth rate\
19. **pop.gwth.urban**: Total urban population growth rate\
20. **gdp.dflt**: GDP deflator\
21. **gdr.eql**: Gender equality rating\
22. **gcf**: Gross Capital Formation\
23. **trade**: Trade = import + export (% of GDP)
24. **gdp.pc**: GDP per capita (current US$)

Data imports and combining:

```{r}
# helper functions
importWDI <- function(filepath, value_name) {
  df <- read_csv(filepath, skip = 4) 
  
  colnames(df) <- tolower(gsub(" ", ".", colnames(df)))
  
  df <- df %>% 
    pivot_longer(5:ncol(.), names_to = "year", values_to = "value") %>% 
    filter(!is.null(value) & !is.na(value)) %>% 
    mutate(country.code = factor(country.code), 
           country.name = factor(country.name),
           year = as.numeric(year)) %>% 
    select(country.code, country.name, year, value)
  
  colnames(df)[4] <- value_name
  
  df
}

importRegionClass <- function(filepath) {
  df <- read_csv(filepath, skip = 4) 
  
  colnames(df) <- tolower(gsub(" ", ".", colnames(df)))
  
  df %>% mutate(country.name = factor(country.name),
                region = factor(region)) %>% 
    select(country.name, reg = region)
}

importIncomeClass <- function(filepath) {
  df <- read_csv(filepath, skip = 4) 
  
  colnames(df) <- tolower(gsub(" ", ".", colnames(df)))
  
  df %>% 
    pivot_longer(3:ncol(.), names_to = "year", values_to = "income") %>% 
    filter(!is.null(income) & !is.na(income)) %>% 
    mutate(country.code = factor(country.code), 
           country.name = factor(country.name), 
           year = as.numeric(year), 
           income = factor(income)) %>% 
    select(country.code, country.name, year, income)
}
```

```{r, message = F}
# import data
setwd("../data")

poverty.headcount <- importWDI("poverty.headcount.215dollar.csv", "pov")
mpi <- importWDI("mpi.csv", "mpi")
education.expenditure.total <- importWDI("total.education.expenditure.csv", "edu.total")
education.expenditure.primary <- importWDI("primary.education.expenditure.csv", "edu.pri")
education.expenditure.secondary <- importWDI("secondary.education.expenditure.csv", "edu.sec")
education.expenditure.tertiary <- importWDI("tertiary.education.expenditure.csv", "edu.ter")
health.expenditure <- importWDI("health.expenditure.csv", "hlth")
military.expenditure <- importWDI("military.expenditure.csv", "mil")
fdi <- importWDI("fdi.csv", "fdi")
labour.force.participation <- importWDI("labour.force.participation.csv", "lbr.part")
unemployment.rate <- importWDI("unemployment.csv", "unemp")
population.growth <- importWDI("population.growth.csv", "pop.gwth.total")
rural.population.growth <- importWDI("rural.population.growth.csv", "pop.gwth.rural")
urban.population.growth <- importWDI("urban.population.growth.csv", "pop.gwth.urban")
gdp.deflator <- importWDI("gdp.deflator.csv", "gdp.dflt")
gender.equality <- importWDI("gender.equality.csv", "gdr.eql")
gross.capital.formation <- importWDI("gross.capital.formation.csv", "gcf")
trade <- importWDI("trade.csv", "trade")
region.class <- importRegionClass("region.class.csv")
income.class <- importIncomeClass("income.class.csv")
gdp.pc <- importWDI("gdp.pc.csv", "gdp.pc")

setwd("../src")
```

We found that the data sets collected from [World Bank's Data helpdesk](https://datahelpdesk.worldbank.org/knowledgebase/articles/906519-world-bank-country-and-lending-groups) and [The World Bank's Data](https://data.worldbank.org/) have different naming convention for certain countries (e.g. "Czechia" vs. "Czechnia Republic"). So we need to rename these countries to avoid some error when joining.

Furthermore, WDI's data sets rate also account for non-country (e.g. country.name = "Low income" or "South Asia"). These special groups are not in our scope of interest, which is national, so we eliminate them.

```{r}
# using poverty.headcount as a naming standard (as other data from WDI also use this convention)
# join a subset of data to process the names
d <- poverty.headcount %>%
  select(country.name) %>%
  mutate(inPov = T) %>%  
  full_join(income.class %>% 
              select(country.name) %>% 
              mutate(inIncome = T), by = "country.name") %>%  
  full_join(region.class %>%
              select(country.name) %>% 
              mutate(inReg = T), by = "country.name") %>% 
  mutate(inPov = !is.na(inPov), inIncome = !is.na(inIncome), inReg = !is.na(inReg))

d
```

First, remove special economic groups from `poverty.headcount`. We figured these regions will not appear in `income.class` or `region.class`, so we might find something from looking at the countries **only** appear in `poverty.headcount`.

```{r}
d %>% filter(inPov & (!inIncome | !inReg)) %>% distinct(country.name)
```

Lucky! We can look through these 18 results and compose a list of special regions.

```{r}
spec.reg <- c("Fragile and conflict affected situations", "IDA total", "World", "East Asia & Pacific", "Europe & Central Asia", "Latin America & Caribbean", "Middle East & North Africa", "South Asia", "Sub-Saharan Africa", "Low income", "Low & middle income", "Lower middle income", "Upper middle income", "High income")
```

Then, we rename those countries with inconsistent naming convention. Since we should only care about countries whose poverty headcount is available, reusing the list generated above, we can identify:

1.  Cote d'Ivoire (also Côte d'Ivoire)
2.  Czechia (also Czechoslovakia or Czech Republic)
3.  Curacao (also Curaçao)
4.  Turkiye (formerly known as Turkey, also Türkiye)
5.  Sao Tome and Principe (also São Tomé and Príncipe)

```{r}
# mapping standard name and variation
nameMap <- tibble(standard = c("Cote d'Ivoire", "Czechia", "Czechia", "Curacao", "Turkiye", "Turkiye", "Sao Tome and Principe"), variation = c("Côte d'Ivoire", "Czechoslovakia", "Czech Republic", "Curaçao", "Turkey", "Türkiye", "São Tomé and Príncipe"))

correctName <- function(name) {
  tibble(name = name) %>% 
    left_join(nameMap, by = c("name" = "variation")) %>% 
    mutate(standard = ifelse(is.na(standard), name, standard)) %>% 
    select(standard) %>% 
    pull()
}

orig.name <- c("Vietnam", "China", "Turkey", "Czechia Republic")
correctName(orig.name)
```

Let's test this out!

```{r}
d <- poverty.headcount %>%
  # correct name here
  mutate(country.name = correctName(country.name)) %>% 
  select(country.name) %>%
  mutate(inPov = T) %>%
  full_join(income.class %>% 
              # correct name here
              mutate(country.name = correctName(country.name)) %>% 
              select(country.name) %>% 
              mutate(inIncome = T), by = "country.name") %>%  
  full_join(region.class %>%
              # correct name here
              mutate(country.name = correctName(country.name)) %>% 
              select(country.name) %>% 
              mutate(inReg = T), by = "country.name") %>% 
  filter(!(country.name %in% spec.reg)) %>% 
  mutate(inPov = !is.na(inPov), inIncome = !is.na(inIncome), inReg = !is.na(inReg))

# countries not in region list, but is in Pov list
d %>% 
  filter(!inReg & inPov) %>% 
  distinct(country.name) %>% 
  nrow()
# countries not in income list, but is in Pov list
d %>% 
  filter(!inIncome & inPov) %>% 
  distinct(country.name) %>% 
  nrow()
```

We are *pretty* confident that there's no inconsistent naming left unprocessed in the data sets.

```{r}
# Rename countries in all data sets.
poverty.headcount <- poverty.headcount %>%
  mutate(country.name = correctName(country.name))
mpi <- mpi %>%
  mutate(country.name = correctName(country.name))
education.expenditure.total <- education.expenditure.total %>%
  mutate(country.name = correctName(country.name))
education.expenditure.primary <- education.expenditure.primary %>%
  mutate(country.name = correctName(country.name))
education.expenditure.secondary <- education.expenditure.secondary %>%
  mutate(country.name = correctName(country.name))
education.expenditure.tertiary <- education.expenditure.tertiary %>%
  mutate(country.name = correctName(country.name))
health.expenditure <- health.expenditure %>%
  mutate(country.name = correctName(country.name))
military.expenditure <- military.expenditure %>%
  mutate(country.name = correctName(country.name))
fdi <- fdi %>%
  mutate(country.name = correctName(country.name))
labour.force.participation <- labour.force.participation %>%
  mutate(country.name = correctName(country.name))
unemployment.rate <- unemployment.rate %>%
  mutate(country.name = correctName(country.name))
population.growth <- population.growth %>%
  mutate(country.name = correctName(country.name))
rural.population.growth <- rural.population.growth %>%
  mutate(country.name = correctName(country.name))
urban.population.growth <- urban.population.growth %>%
  mutate(country.name = correctName(country.name))
gdp.deflator <- gdp.deflator %>%
  mutate(country.name = correctName(country.name))
gender.equality <- gender.equality %>%
  mutate(country.name = correctName(country.name))
gross.capital.formation <- gross.capital.formation %>%
  mutate(country.name = correctName(country.name))
trade <- trade %>%
  mutate(country.name = correctName(country.name))
region.class <- region.class %>%
  mutate(country.name = correctName(country.name))
income.class <- income.class %>%
  mutate(country.name = correctName(country.name))
gdp.pc <- gdp.pc %>%
  mutate(country.name = correctName(country.name))
```

Join the data

```{r, message = F}
countries <- poverty.headcount %>% 
  # We used a full join here so we can conduct a separate analysis on mpi later
  full_join(mpi, by = c("country.name", "country.code", "year")) %>%
  left_join(income.class, c("country.name", "country.code", "year")) %>% 
  left_join(region.class, by = "country.name") %>%
  left_join(education.expenditure.total, by = c("country.name", "country.code", "year")) %>%
  left_join(education.expenditure.primary, by = c("country.name", "country.code", "year")) %>%
  left_join(education.expenditure.secondary, by = c("country.name", "country.code", "year")) %>%
  left_join(education.expenditure.tertiary, by = c("country.name", "country.code", "year")) %>%
  left_join(health.expenditure, by = c("country.name", "country.code", "year")) %>%
  left_join(military.expenditure, by = c("country.name", "country.code", "year")) %>%
  left_join(fdi, by = c("country.name", "country.code", "year")) %>%
  left_join(labour.force.participation, by = c("country.name", "country.code", "year")) %>%
  left_join(unemployment.rate, by = c("country.name", "country.code", "year")) %>%
  left_join(population.growth, by = c("country.name", "country.code", "year")) %>%
  left_join(rural.population.growth, by = c("country.name", "country.code", "year")) %>%
  left_join(urban.population.growth, by = c("country.name", "country.code", "year")) %>%
  left_join(gdp.deflator, by = c("country.name", "country.code", "year")) %>%
  left_join(gender.equality, by = c("country.name", "country.code", "year")) %>%
  left_join(gross.capital.formation, by = c("country.name", "country.code", "year")) %>%
  left_join(trade, by = c("country.name", "country.code", "year")) %>% 
  left_join(gdp.pc, by = c("country.name", "country.code", "year")) %>% 
  # filter special groups
  filter(!(country.name %in% spec.reg))
```

Data preview

```{r}
head(countries)
str(countries)
summary(countries)
```

There's no NA in `reg`, which is a sign that all naming in the data is remedied. There's some expected NAs in `income` and `pov`, as these data are collected by year. There's a substantial amount of missing data in `mpi`, as this is a relative new concept. We will address the nature, and processing of missing data in the next sections.

## 2.2. Missing values

As observed from the summary above, the data set contains a lot of missing values in some of the variables.

```{r}
mean(is.na(countries))
```

About 19% of the data set is missing.

```{r}
nCompleteObs <- sum(complete.cases(countries))
print(paste("No. of complete cases:", nCompleteObs))
```

There are only 3 complete cases where all the variable is available. This is nowhere near acceptable to conduct any meaningful analysis. Therefore, we need to eliminate some variables for a more balance data set.

```{r}
missings <- colMeans(is.na(countries))
ggplot(mapping = aes(x = names(missings), y = missings, fill = missings < 0.35)) +
  geom_bar(stat = "identity") +
  ggtitle("Missing rate") +
  xlab("variables") +
  ylab("% missing") +
  theme(axis.text.x = element_text(size=9, angle=90))

missings[missings > 0.35]
```

There are 5 variables with missing rate \>35%.

expenditure in primary, secondary, and tertiary edication can be very useful and relevant information to predict poverty reduction [@pubspend&pov]. However, we would like to exclude these variables from some first analyses to make use of the richer set of data. We can conduct a separate analysis with these variable to gain more insight.

```{r}
# variables with high missing rate
hMiss <- names(missings[missings > 0.35])
# exclude these variables in countries1
countries1 <- countries %>% 
  select(!hMiss) %>% 
  filter(!is.na(pov))
str(countries1)
```

Re-evaluate the `countries1` set.

```{r}
mean(is.na(countries1))
sum(complete.cases(countries1))
mean(complete.cases(countries1))
```

On average, each column has 6% missing rate, results in 937 complete data point (i.e. 49%). This can be a sufficient number for the analysis. However, the missing data can induce loss of power due to the reduced sample size, and some other biases depending on which variables is missing.

```{r}
# complete rate of data by regions
countries1 %>% 
  mutate(isComplete = complete.cases(.)) %>% 
  group_by(reg) %>% 
  summarise(complete.rate = mean(isComplete)) %>% 
  arrange(desc(complete.rate))
```

Countries from North America, Sub-Saharan Africa, and South Asia have the highest rate of missing data. We suspect that Sub-Saharan Africa, and South Asia are comparably less accessible regions. We also know that Americans don't like filling out forms, so their high rate of missing data is understandable as well.

Still, we need to find a way to address this issue. we propose several approaches:

1.  **Use complete cases**: Only use the complete cases for the analysis. This is a straightforward approach, but doesn't resolve the bias resulted from the mass loss of data.
2.  **Selectively remove variables with high missing rate**: The same as we did before, but this process should be carried out carefully as we run the chance of dropping an important variable.
3.  **Update the data set as we select variables**: As we drop insignificant variables (in backward selection), the number of NAs are changed as well. We can utilize the extra complete cases to build the next model in the steps.
4.  **Imputation**: The idea is to replace the missing observations on the response or the predictors with artificial values that try to preserve the data set structure. This is a quite complex topic of its own, but we think why not. You can read more from @imputation.

## 2.3. Descriptive Analytics
Distribution of the predicted variable `pov`
```{r}
ggplot(countries, aes(x = pov)) +
  geom_histogram(aes(y = ..density..)) +
  geom_density(alpha = .2, fill = "blue")
```
The graph displays a decreasing rate as poverty indicator increasing. This might not be representative of the current state of poverty in the world, but of the number presented in our data. For example, more recent data is likely to be more inclusive than ancient data, when poverty is more prevalent. We should look at data from the same period.

```{r}
# number of data point available from 1967 to 2021
ggplot(poverty.headcount, aes(x = year)) +
  geom_histogram()

# pov data from 1998 and 2018
pov.98.18 <- poverty.headcount %>%
  filter(year == 1998 | year == 2018)
pov.98.18 %>% 
  group_by(year) %>% 
  summarise(sum = n())

ggplot(pov.98.18, aes(x = pov)) +
  geom_histogram(aes(y = ..density..)) +
  geom_density(alpha = .2, fill = "blue") +
  facet_grid(cols = vars(year))
```

The graph for 1998 has a much gentler slope, meaning poverty was more popular during that time, as predicted from out intuition. What about the general progress of the world?

```{r}
# Re-import pov and only take special regions
# geographic
geo.regs <- c("EAS", "ECS", "LCN", "MEA", "SAS", "SSF", "WLD")
# economics
eco.regs <- c("HIC", "LIC", "LMC", "LMY", "UMC")

pov.reg <- importWDI("../data/poverty.headcount.215dollar.csv", "pov") %>% 
  filter(country.code %in% c(geo.regs, eco.regs)) %>% 
  mutate(type = ifelse(country.code %in% geo.regs, "Geographic", "Economics"))

pov.reg %>%
  distinct(country.code, country.name, type) %>% 
  arrange(type)

# World
ggplot(pov.reg %>% filter(country.code == "WLD"),
       aes(x = year, y = pov)) +
  geom_point() +
  geom_smooth()
```

An overall very steady decrease of poverty. How about each region?

```{r}
ggplot(pov.reg %>% filter(country.code != "WLD"),
       aes(x = year, y = pov, color = country.name)) +
  geom_point() +
  geom_smooth() +
  facet_grid(rows = vars(type))
```

There's a general steady, but distinct decline of poverty over time in each type region of respective type. Latin America & Caribbean, Europe & Central Asia, Middle East & North Africa, and High Income group has a more gradual decline as they are not very poor to begin with.  

Among the income groups, Low & Middle Income, Lower & Middle Income, and Upper Middle Income have quite similar in term of poverty indicator and slope over the year. While these values vary greatly among different geographical regions.  

Let's see some important statistics

```{r}
stats <- poverty.headcount %>% 
  summarise(count = n(), skewness = skewness(pov), kurtosis = kurtosis(pov), std.deviation = sd(pov))

kable(stats)
```

## 2.4. Data Source

- [poverty.headcount](https://data.worldbank.org/indicator/SI.POV.DDAY)
- [mpi](https://data.worldbank.org/indicator/SI.POV.MDIM.XQ)
- [education.expenditure.primary](https://data.worldbank.org/indicator/SE.XPD.PRIM.ZS)
- [education.expenditure.secondary](https://data.worldbank.org/indicator/SE.XPD.SECO.ZS)
- [education.expenditure.tertiary](https://data.worldbank.org/indicator/SE.XPD.TERT.ZS)
- [education.expenditure.total](https://data.worldbank.org/indicator/SE.XPD.TOTL.GD.ZS)
- [health.expenditure](https://data.worldbank.org/indicator/SH.XPD.GHED.GD.ZS)
- [military.expenditure](https://data.worldbank.org/indicator/MS.MIL.XPND.GD.ZS)
- [fdi](https://data.worldbank.org/indicator/BX.KLT.DINV.CD.WD)
- [unemployment.rate](https://data.worldbank.org/indicator/SL.UEM.TOTL.NE.ZS)
- [labour.force.participation](https://data.worldbank.org/indicator/SL.TLF.CACT.NE.ZS)
- [gender.equality](https://data.worldbank.org/indicator/IQ.CPA.GNDR.XQ)
- [population.growth](https://data.worldbank.org/indicator/SP.POP.GROW)
- [urban.population.growth](https://data.worldbank.org/indicator/SP.URB.GROW)
- [rural.population.growth](https://data.worldbank.org/indicator/SP.RUR.TOTL.ZG)
- [gdp.deflator](https://data.worldbank.org/indicator/NY.GDP.DEFL.KD.ZG)
- [gross capital formation](https://data.worldbank.org/indicator/NE.GDI.TOTL.ZS)
- [trade](https://data.worldbank.org/indicator/NE.TRD.GNFS.ZS)
- [region.class](https://datatopics.worldbank.org/world-development-indicators/the-world-by-income-and-region.html)
- [income.class](https://datahelpdesk.worldbank.org/knowledgebase/articles/906519-world-bank-country-and-lending-groups)
- [gross.capital.formation](https://data.worldbank.org/indicator/NE.GDI.TOTL.ZS)

# 3. Model Selection and Interpretation
## 3.1. Assumption Check
We can conduct some preliminary checks on linearity and correlation of predictors to have a better picture of the data. [Checklist](https://godatadrive.com/blog/basic-guide-to-test-assumptions-of-linear-regression-in-r)
**1. Linear relationship:**  
Use correlation matrix to check linearity
```{r, warning = FALSE}
# select only numeric data
countries.num <- countries %>% select(where(is.numeric))
chart.Correlation(countries.num, histogram = TRUE, pch = 19)
```
Which is utterly intelligible, but a majority of the fitted lines are linear at first glance. We should render separate graphs for the relationship between `pov` & other variables.
```{r}
# lengthen data table to variable-value pairs, with pov as predicted variable and others as predictive 
countries.num2 <- countries.num %>% 
  pivot_longer(cols = 3:ncol(.), names_to = "variable", values_to = "value") %>% 
  filter(!is.na(value) & !is.na(pov))

drawGraph <- function(indvar, data) {
  ggplot(data %>% 
           filter(variable == indvar),
         aes(x = value, y = pov)) +
    geom_point() +
    geom_smooth(method = lm) +
    labs(title = paste("Relationship pov ~", indvar),
         x = indvar,
         y = "pov")
}

ivs <- distinct(countries.num2, variable)[[1]]

for (indvar in ivs) {
  print(
    ggplot(countries.num2 %>% 
           filter(variable == indvar),
         aes(x = value, y = pov)) +
    geom_point() +
    geom_smooth(method = lm) +
    labs(title = paste("Relationship pov ~", indvar),
         x = indvar,
         y = "pov")
  )
}
```

Most variables display linear relationship with some exceptions, which are more appropriate to assume a logarithmic relationship. Gender equality should be viewed as a categorical data.
```{r}
logIvs <- c("fdi", "gdp.dflt", "gdp.pc")

for (indvar in logIvs) {
  print(
    ggplot(countries.num2 %>% 
           filter(variable == indvar),
         aes(x = log(value), y = pov)) +
    geom_point() +
    geom_smooth(method = lm) +
    labs(title = paste0("Relationship pov ~ log(", indvar, ")"),
         x = paste0("log(", indvar, ")"),
         y = "pov")
  )
}
```

The relationship are more linear after the transformation.

```{r}
gdr.eql <- countries.num2 %>% 
  filter(variable == "gdr.eql") %>% 
  mutate(value = factor(value))

ggplot(gdr.eql,
       aes(x = value, y = pov)) +
  geom_boxplot() +
  geom_smooth(method = lm) +
  labs(title = paste("Relationship pov ~ gdr.eql"),
       x = "gdr.eql",
       y = "pov")
```
There's a significant correlation between gender equality and poverty.  
Correlation coefficients between `pov` and predictors.

```{r}
# transform some variables
countries.num3 <- countries.num %>% 
  mutate(lfdi = log(fdi), lgdp.dflt = log(gdp.dflt), lgdp.pc = log(gdp.pc)) %>% 
  mutate(lfdi = ifelse(is.nan(lfdi) | lfdi == -Inf, NA, lfdi))

# name of the independent variables
cn <- colnames(countries.num3)[-c(1,2)]

# correlation of independent variable and pov
corWithPov <- function(indevar, data) {
  cor(data[[indevar]], data[["pov"]], use = "complete.obs")
}

cor.pov <- sapply(cn, corWithPov, data = countries.num3)
kable(cor.pov)
```

`edu.ter`, `mil`, `fdi`, `lbr.part`, `unemp`, `gdp.dflt`, `gcf` have negligible correlation with `pov`. `lgdp.pc`, `lfdi`, and `lgdp.dflt` have stronger linear relationship with `pov` than their un-transformed counterparts.

```{r}
countries1 <- countries1 %>% 
  mutate(lfdi = log(fdi), lgdp.dflt = log(gdp.dflt), lgdp.pc = log(gdp.pc)) %>% 
  mutate(lfdi = ifelse(is.nan(lfdi) | lfdi == -Inf, NA, lfdi))
```


**2. Predictors are independent**
There should be no correlation/multicolinearity between each pair of predictors.

```{r}
countries.arr <- simplify2array(countries.num %>% 
                                  select(-c("year", "pov")))

cor.mtx <- rcorr(countries.arr, type = "spearman")

round(cor.mtx$r, 2)

corrplot(cor.mtx$r, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)
```
Some significant correlations can be found between `gdr.eql` with `edu.pri`, `pop.gwth.total`, and `fdi` with `mil`, etc. We expect these variables to be eliminated in the VIF test. This might be a good property for imputation (3.2.4)

## 3.2. Ordinary Multiple Linear Regression

We conduct a normal linear regression, following the approaches mentioned above to address missing values issues.

### 3.2.1. Use Complete Cases (To be done)
#### 3.2.1.1. Model Fitting
#### 3.2.1.2. Assessment
#### 3.2.1.3. Interpretation

```{r}
library(tidyverse) # for data manipulations and visualisations
library(ggpubr) # for correlation coefficient on plot
library(gridExtra) # for arranging plots 
library(e1071) # for skewness
library(ggplot2)
library(dplyr)
library(tidyr)
library(readxl)
library(corrplot)
library(car)
library(DescTools)
library(glmnet)
```

First we will write the function "eval_results(true,predict) to evaluate the models
```{r}
eval_results <- function(true, predict) {
  actual <- data.matrix(true)
  SSE <- sum((predict - actual)^2)
  SST <- sum((actual - mean(actual))^2)
  R_square <- 1 - SSE / SST
  data.frame(
    MSE = MSE(predict, true),
    MAE = MAE(predict, true),
    RMSE = RMSE(predict, true),
    Rsquare = R_square
  )
}
```
First we read the csv file for the countries containing the dataset. We will then put the variables we need in to subset1.
```{r}
df <-read.csv('../data/countries1.csv')
df
subset1 <- df %>% select("pov","country.code","year","income","edu.total","hlth","mil","fdi","lbr.part","unemp","pop.gwth.total","pop.gwth.rural","gdp.dflt","gcf","trade","gdp.pc","lfdi","lgdp.dflt","lgdp.pc")
subset1
```


Next we will build a SLR model. We will put 80% of dataset to train and the rest to test.
```{r}
set.seed(10)
index <- sort(sample(x = nrow(subset1), size = nrow(subset1)*.8))
train <- df[index, ]
test <- df[-index, ]
```

The model output will be placed in lm.slr and can be viewed in summary(lm.slr). THe r-squared value is 0.9619.
```{r}
lm.slr <- lm(pov ~ country.code+year+income+edu.total+hlth+mil+fdi+lbr.part+unemp+pop.gwth.total+pop.gwth.rural+gdp.dflt+gcf+trade+gdp.pc+lfdi+lgdp.dflt+lgdp.pc,data = train)
summary(lm.slr)

```
Next we will be performing standardisation.However not all the variables are numerical. Therefore to overcome it we build the function below. This will allow us to perform standardisation to the dataset.
```{r}
subset1$income <- factor(subset1$income)
subset1$country.code <- factor(subset1$country.code)



sd0 <- function(vct) {
  if (!is.numeric(vct)) {
    return(NA)
  }
  
  return(sd(vct, na.rm = T))
}


std0 <- function(vct, scl) {
  if (!is.numeric(vct)) {
    return(vct)
  }
  
  return(vct/scl)
}

scale <- function(data) {
  unlist(lapply(data, sd0))
}

standardise <- function(data) {
  scaler <- scale(data)
  
  numCols <- which(unlist(lapply(data, is.numeric)))
  num <- as.data.frame(mapply(std0, vct = data[,numCols], scl = scaler[numCols], SIMPLIFY = T))
  fct <- data[,-numCols]
  return(cbind(fct, num))
}
```

Next we will perform standardistation to the dataset.
```{r}
str(subset1)
df0 <- subset1
subset1
apply(subset1, 2, sd)


```

Next we will plot a pov vs income scatter plot to see the effect of income on pov. From the scatter plot, we can deduce that the low income countries have a higher pov while the higher income countries have low pov. This is consistent with our intuition that countries that are richer do not have as much poverty issue as the poorer countries.
```{r}
ggplot(data = subset1, aes(x = income, y = pov)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(title = "Pov vs. Income ",
       x = "income",
       y = "pov")

```
Next we will build a MLR model with data from subset 1 to see the relationship between dependent variable:pov and the other variables

```{r}
model1 <- lm(pov ~ ., data = subset1)
summary(model1)
vif(model1)

```
Next we will perform regularisation. We will split the data to predictor variable "train.x" and response variables "train.y"
```{r}
subset2 <- subset(subset1,select =c("pov","country.code","year","income","edu.total","hlth","mil","fdi","lbr.part","unemp","pop.gwth.total","pop.gwth.rural","gdp.dflt","gcf","trade","gdp.pc","lfdi","lgdp.dflt","lgdp.pc"))
subset2 <- na.omit(subset2)
subset2
train.x <- data.matrix(subset(subset2, select = -pov))
train.y = data.matrix(subset2$pov)
```

Next we will perform splitting of data to train and test.
```{r}
split_train_test <- function(data) {
set.seed(1984)
# split data
seeds <- data %>% 
  group_by(country.code) %>% 
  filter(row_number() == 1)

plants <- data %>% 
  group_by(country.code) %>% 
  filter(row_number() != 1)

isComplete <- which(complete.cases(plants))
idx <- sample(isComplete, replace = F, 0.2 * nrow(plants))

training <- plants[-idx,] %>% 
  rbind(seeds)
test <- plants[idx,]

return(list(training, test))
}

training <- split_train_test(subset2)[[1]]
test <- split_train_test(subset2)[[2]]

ntrain <- 1:nrow(training)
combine <- rbind(training, test)
combine.mtrx <- model.matrix(pov ~ ., data = combine)
train.mtrx <- combine.mtrx[ntrain,]
test.mtrx <- combine.mtrx[-ntrain,]

train.x <- train.mtrx[,-1]
train.y <- training$pov

test.x <- test.mtrx[,-1]
text.y <- test$pov
```

We then build a MLR model with the training model to train it.

```{r}
model3 <- lm(pov ~., data = training)
summary(model3)

```

```{r}
fit <- model3$fitted.values
true <- training$pov
summary_MLR3_train <- eval_results(true, fit)
```

We then evaluate the model between train and test
```{r}
fit <- predict(model3, newdata=test)
true <- test$pov
summary_MLR3_test <- eval_results(true, fit)
```

We can see the evaluation of the model from the function below. From the train model, the R^squared value will be 0.965 and for the test model it will be 0.881.

```{r}
summary_MLR <- rbind(summary_MLR3_train[-5], 
                     summary_MLR3_test[-5])
rownames(summary_MLR) <- c("Baseline MLR_Train", 
                           "Baseline MLR_Test" )
knitr::kable(summary_MLR, digits = 3)
```
Next we will be performing ridge and lasso regression model using the following code. We will split the data to train and test.
```{r}
# Training Data
data <- training
train.x <- data.matrix(data[,-ncol(data)])
train.y <- data.matrix(data[,ncol(data)])

# Test Data
data <- test
test.x <- data.matrix(data[,-ncol(data)])
test.y <- data.matrix(data[,ncol(data)])
```

Next we will perform the Ridge regression model.
```{r}
# Fit Ridge Regression Model
set.seed(123)
cv_ridge <- cv.glmnet(train.x, train.y, alpha = 0)
glm_ridge <- glmnet(train.x, train.y, 
                    alpha = 0, lambda = cv_ridge$lambda.min)

# Evaluate Ridge Regression Model on Training Set
fit <- predict(glm_ridge, newx = train.x, type = "response")
true <- train.y
summary_ridge_train <- eval_results(true, fit)


# Evaluate Ridge Regression Model on Test Set
fit <- predict(glm_ridge, newx = test.x, type = "response")
true <- test.y
summary_ridge_test <- eval_results(true, fit)

# Summarise Ridge Regression Model Performance into Table
summary_ridge <- rbind(summary_ridge_train, summary_ridge_test)
rownames(summary_ridge) <- c("Ridge Model_Train", "Ridge Model_Test" )
```

Next we will perform Lasso Regression model

```{r}
# Fit LASSO Model
set.seed(123)
cv_lasso <- cv.glmnet(train.x,train.y, alpha = 1)
glm_lasso <- glmnet(train.x,train.y, alpha = 1, 
                    lambda = cv_lasso$lambda.min)

# Evaluate LASSO Model on Training Set
fit <- predict(glm_lasso, newx = train.x)
true <- train.y
summary_lasso_train <- eval_results(true, fit)

# Evaluate LASSO Model on Test Set
fit <- predict(glm_lasso, newx = test.x)
true <- test.y
summary_lasso_test <- eval_results(true, fit)

# Summarise LASSO Model Performance into Table
summary_lasso <- rbind(summary_lasso_train, summary_lasso_test)
rownames(summary_lasso) <- c("LASSO Model_Train", "LASSO Model_Test" )
```

We will compare the 3 models that we have build, Multi linear ,Ridge and Lasso regression models. We evaluate the 3 models together and compare both train and test models of each of the 3 regression model.

```{r}
summary_3models <- rbind(summary_MLR3_train, 
                         summary_ridge_train,                               
                         summary_lasso_train, 
                         summary_MLR3_test,                             
                         summary_ridge_test, 
                         summary_lasso_test)

rownames(summary_3models) <- c("Baseline MLR_Train", "Ridge Model_Train", 
                               "LASSO Model_Train",  "Baseline MLR_Test",  
                               "Ridge Model_Test", "LASSO Model_Test" )

knitr::kable(summary_3models[1:3,-5], digits = 3)
knitr::kable(summary_3models[4:6,-5], digits = 3)

```
From the 3 models we can see that the original multi linear regression model is the best one with the highest R^squared value. However the Lasso Regression Model has the lowest MSE, MAE and RMSE. We can conclude that the reason for the first model having the highest R^squared value might be due to overfitting as many values will get removed. Next we will build other models to see if they work better.

### 3.2.2. Selectively remove variables with high missing rate (To be done)
#### 3.2.2.1. Model Fitting
#### 3.2.2.2. Assessment
#### 3.2.2.3. Interpretation

### 3.2.3. Update the data set as we select variables (To be done)
#### 3.2.3.1. Model Fitting
#### 3.2.3.2. Assessment
#### 3.2.3.3. Interpretation

### 3.2.4. Imputation
**1. What is imputation**  
Imputation is a technique to handle missing values by replacing missing data with substitute values. In our study, we focus on "item imputation", which means substituting for a component of a data point (i.e. a variable). The general idea is to take a value that preserve the property of the data (e.g., distribution, mean, standard deviation), or predict the value using other variables.  

We use **Multivariate Imputation By Chained Equations (MICE)** as our primary tool for this technique, because of its wide acceptance in scientific studies (@imputWorks). In order to use this technique, we have to assume that the "missingness" of a field can be explained by the values in other columns, (e.g., If the countries is in North America, it's more likely to be missing as we have discovered in 2.2). The general ideas of the algorithm is iteratively predict the missing values based on other variables, and improve it after each iterations until predicted value converse to a stable point. The algorithmic details of MICE is very concisely explained in @micealgo.  

Our parameter for this algorithm is fairly standard:  

- `m = 5` imputed data sets
- `maxit = 30` iterations

The imputation method is `cart` (Classification and Regression Trees).

```{r}
set.seed(1984)
# split data
isComplete <- which(complete.cases(countries1))
idx <- sample(isComplete, replace = F, 0.3 * nrow(countries1))
countries.train.4 <- countries1[-idx,]
countries.test.4 <- countries1[idx,]

# blank for no imputation, cart for variable requiring imputation
meth <- c(rep("", 4), "cart", "", rep("cart", 16))
names(meth) <- colnames(countries1)
meth

# run the algorithm
# countries1.imputed <- mice(countries1, m = 3, maxit = 20, method = meth)
# saveRDS(countries1.imputed, 'countries1.imputed.RData')
countries1.imputed <- readRDS('countries1.imputed.RData')
summary(countries1.imputed)
```
We can take a look into one of the imputated data sets.
```{r}
countries1.imputed.1 <- complete(countries1.imputed, 1)
summary(countries1.imputed.1)
sum(!complete.cases(countries1.imputed.1))
```
We found no missing cases as expected.

#### 3.2.4.1. Model Fitting
##### A. Ordinary Linear Regression
We generated 5 sets of imputated (training) data. We can build our model on each of these data sets and combine the estimates using [pooling rule](https://amices.org/mice/reference/pool.html#details). With all the generated data sets.

```{r}
formula.str <- 'pov ~ reg+year+income+edu.total+hlth+mil+fdi+lbr.part+unemp+pop.gwth.total+pop.gwth.rural+pop.gwth.urban+gdp.dflt+gcf+trade+gdp.pc+lfdi+lgdp.dflt+lgdp.pc'

# Using only the first imputed data set
fit1 <- lm(as.formula(formula.str), data = complete(countries1.imputed, 1))
summary(fit1)
fit1.pred <- predict(fit1, countries.test.4)

# Build models with all generated data set and pool the estimates
fit2 <- with(data = countries1.imputed, exp = lm(as.formula(formula.str)))
fit2.combined <- pool(fit2)
summary(fit2.combined)
# dummy lm model
fit2.dummy <- lm(as.formula(formula.str), data = countries1)
# replace coefficients of dummy model & predict
fit2.dummy$coefficients <- fit2.combined$pooled$estimate
fit2.pred <- predict(fit2.dummy, countries.test.4)
```
Helper function to calculate R-Squared
```{r}
r2 <- function(pred, orig) {
   RSS <- sum((pred - orig)^2)
   TSS <- sum((pred - mean(orig))^2)
   R2 <- 1 - RSS/TSS
   return (R2)
}

adjR2 <- function(pred, orig, k) {
  R2 <- r2(pred, orig)
  n <- length(pred)
  adjr2 <- 1 - (1-R2)*(n-1)/(n-k-1)
  return (adjr2)
}
```
Adjusted R-Squared on train and test
```{r}
k <- length(fit1$coefficients) - 1
fit1.train.r2 <- summary(fit1)$adj.r.squared
fit1.test.r2 <- adjR2(fit1.pred, countries.test.4$pov, k)
fit2.train.r2 <- pool.r.squared(fit2, adjusted = T)[,'est']
fit2.test.r2 <- adjR2(fit2.pred, countries.test.4$pov, k)
res <- data.frame(model = c("Single", "Pooled"), train = c(fit1.train.r2, fit2.train.r2), test = c(fit1.test.r2, fit2.test.r2))
kable(res)
```

The models seem to be over-fitting. We should perform model simplification.

##### B. Regularization

Prepare data

```{r}
sd0 <- function(vct) {
  if (!is.numeric(vct)) {
    return(NA)
  }
  
  return(sd(vct, na.rm = T))
}


std0 <- function(vct, scl) {
  if (!is.numeric(vct)) {
    return(vct)
  }
  
  return(vct/scl)
}

standardise <- function(train, test) {
  data <- rbind(train, test)
  scaler <- unlist(lapply(data, sd0))
  
  numCols <- which(unlist(lapply(data, is.numeric)))
  num <- as.data.frame(mapply(std0, vct = data[,numCols], scl = scaler[numCols], SIMPLIFY = T))
  fct <- data[,-numCols]
  return(cbind(fct, num))
}
```


```{r}
optimalModel <- function(train) {
  #standardized
}
```


##### B. Step-wise AIC
We can conduct a step-wise AIC variable selection. It is similar to the procedure we use in class, but based on a metric call AIC (Akaike Information Criterion), which is an estimator of prediction error and relative quality of statistical models. The lower AIC is, the better the model fits. \\
We have no idea how to pool coefficients for AIC model,

```{r}
# helper
fitAIC <- function(i) {
  fit <- lm(as.formula(formula.str), complete(countries1.imputed, i))
  aic.fit <- stepAIC(fit, trace = F, direction = 'backward')
  return(aic.fit)
}

# build models
aic.fits <- lapply(1:countries1.imputed$m, fitAIC)
# predict with each model
aic.pred <- lapply(aic.fits, predict, newdata = countries.test.4)
# calculate adj r2
aic.train.r2 <- unlist(lapply(aic.fits, \(model) summary(model)$adj.r.squared))

k <- lapply(aic.fits, \(m) length(m$coefficients))
aic.test.r2 <- unlist(mapply(adjR2, aic.pred, k = k, MoreArgs = list(orig = countries.test.4$pov)))

res <- data.frame(`set no.` = 1:3, train = aic.train.r2, test = aic.test.r2)
```

Adjusted R-Squared

```{r}
kable(res)
```



-> AIC on one model [link](https://rforpoliticalscience.com/2020/10/23/choose-model-variables-by-aic-in-a-stepwise-algorithm-with-the-mass-package-in-r/)  


-> stepwise selection on mice  [link](https://stefvanbuuren.name/fimd/sec-stepwise.html)

#### 3.2.4.2. Assessment
Check assumption
outliers
#### 3.2.4.3. Interpretation

## 3.3. Panel Data Analysis (To be done)

# 4. Conclusion
# 5. Appendix
# 6. References
