---
title: "Draft Documents"
author: "Team 1 (Hedgehog, Callista, Imtiyaaz, Issac)"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
  pdf_document: default
bibliography: citations.bib
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F)
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 50), tidy = TRUE)
```

```{r, message = F}
library(needs)

needs("dplyr", "MASS", "knitr", "readr", "tidyr", "ggplot2", "e1071", "moments", "corrplot", "Hmisc", "PerformanceAnalytics", "mice", "car", "glmnet")

prioritize(dplyr)
```

# 1. Introduction (To be done)

# 2. Data Characteristic

## 2.1. Nature of Data

The data set is collection The World Bank Data, the variables of interest are extracted from the raw data files and combined into a single data frame for analysis. The final data set includes:

1.  **country.code**: Country code\
2.  **country.name**: Country name\
3.  **year**: Year\
4.  **income**: Income class
-   Low income (L)\
-   Lower middle income (LM)\
-   Upper middle income (UM)\
-   High income (H)\
5.  **reg**: Region\
6.  **pov**: Poverty headcount ratio based on cut-off value of $2.15 per day\
7.  **mpi**: Multidimensional Poverty Index\
8.  **edu.total**: Total expenditure on education (% of GDP)\
9.  **edu.pri**: Total expenditure on primary education (% of total education expenditure)\
10. **edu.sec**: Total expenditure on secondary education (% of total education expenditure)\
11. **edu.ter**: Total expenditure on tertiary education (% of total education expenditure)\
12. **hlth**: Total expenditure on health (% of GDP)\
13. **mil**: Total expenditure on military (% of GDP)\
14. **fdi**: Foreign Direct Investment\
15. **lbr.part**: Labour force participation (% of population ages 15+)\
16. **unemp**: Unemployment rate\
17. **pop.gwth.total**: Total population growth rate\
18. **pop.gwth.rural**: Total rural population growth rate\
19. **pop.gwth.urban**: Total urban population growth rate\
20. **gdp.dflt**: GDP deflator\
21. **gdr.eql**: Gender equality rating\
22. **gcf**: Gross Capital Formation\
23. **trade**: Trade = import + export (% of GDP)
24. **gdp.pc**: GDP per capita (current US$)

Data imports and combining:

```{r}
# helper functions
importWDI <- function(filepath, value_name) {
  df <- read_csv(filepath, skip = 4) 
  
  colnames(df) <- tolower(gsub(" ", ".", colnames(df)))
  
  df <- df %>% 
    pivot_longer(5:ncol(.), names_to = "year", values_to = "value") %>% 
    filter(!is.null(value) & !is.na(value)) %>% 
    mutate(country.code = factor(country.code), 
           country.name = factor(country.name),
           year = as.numeric(year)) %>% 
    select(country.code, country.name, year, value)
  
  colnames(df)[4] <- value_name
  
  df
}

importRegionClass <- function(filepath) {
  df <- read_csv(filepath, skip = 4) 
  
  colnames(df) <- tolower(gsub(" ", ".", colnames(df)))
  
  df %>% mutate(country.name = factor(country.name),
                region = factor(region)) %>% 
    select(country.name, reg = region)
}

importIncomeClass <- function(filepath) {
  df <- read_csv(filepath, skip = 4) 
  
  colnames(df) <- tolower(gsub(" ", ".", colnames(df)))
  
  df %>% 
    pivot_longer(3:ncol(.), names_to = "year", values_to = "income") %>% 
    filter(!is.null(income) & !is.na(income)) %>% 
    mutate(country.code = factor(country.code), 
           country.name = factor(country.name), 
           year = as.numeric(year), 
           income = factor(income)) %>% 
    select(country.code, country.name, year, income)
}
```

```{r, message = F}
# import data
setwd("../data")

poverty.headcount <- importWDI("poverty.headcount.215dollar.csv", "pov")
mpi <- importWDI("mpi.csv", "mpi")
education.expenditure.total <- importWDI("total.education.expenditure.csv", "edu.total")
education.expenditure.primary <- importWDI("primary.education.expenditure.csv", "edu.pri")
education.expenditure.secondary <- importWDI("secondary.education.expenditure.csv", "edu.sec")
education.expenditure.tertiary <- importWDI("tertiary.education.expenditure.csv", "edu.ter")
health.expenditure <- importWDI("health.expenditure.csv", "hlth")
military.expenditure <- importWDI("military.expenditure.csv", "mil")
fdi <- importWDI("fdi.csv", "fdi")
labour.force.participation <- importWDI("labour.force.participation.csv", "lbr.part")
unemployment.rate <- importWDI("unemployment.csv", "unemp")
population.growth <- importWDI("population.growth.csv", "pop.gwth.total")
rural.population.growth <- importWDI("rural.population.growth.csv", "pop.gwth.rural")
urban.population.growth <- importWDI("urban.population.growth.csv", "pop.gwth.urban")
gdp.deflator <- importWDI("gdp.deflator.csv", "gdp.dflt")
gender.equality <- importWDI("gender.equality.csv", "gdr.eql")
gross.capital.formation <- importWDI("gross.capital.formation.csv", "gcf")
trade <- importWDI("trade.csv", "trade")
region.class <- importRegionClass("region.class.csv")
income.class <- importIncomeClass("income.class.csv")
gdp.pc <- importWDI("gdp.pc.csv", "gdp.pc")

setwd("../src")
```

We found that the data sets collected from [World Bank's Data helpdesk](https://datahelpdesk.worldbank.org/knowledgebase/articles/906519-world-bank-country-and-lending-groups) and [The World Bank's Data](https://data.worldbank.org/) have different naming convention for certain countries (e.g. "Czechia" vs. "Czechnia Republic"). So we need to rename these countries to avoid some error when joining.

Furthermore, WDI's data sets rate also account for non-country (e.g. country.name = "Low income" or "South Asia"). These special groups are not in our scope of interest, which is national, so we eliminate them.

```{r}
# using poverty.headcount as a naming standard (as other data from WDI also use this convention)
# join a subset of data to process the names
d <- poverty.headcount %>%
  select(country.name) %>%
  mutate(inPov = T) %>%  
  full_join(income.class %>% 
              select(country.name) %>% 
              mutate(inIncome = T), by = "country.name") %>%  
  full_join(region.class %>%
              select(country.name) %>% 
              mutate(inReg = T), by = "country.name") %>% 
  mutate(inPov = !is.na(inPov), inIncome = !is.na(inIncome), inReg = !is.na(inReg))

d
```

First, remove special economic groups from `poverty.headcount`. We figured these regions will not appear in `income.class` or `region.class`, so we might find something from looking at the countries **only** appear in `poverty.headcount`.

```{r}
d %>% filter(inPov & (!inIncome | !inReg)) %>% distinct(country.name)
```

Lucky! We can look through these 18 results and compose a list of special regions.

```{r}
spec.reg <- c("Fragile and conflict affected situations", "IDA total", "World", "East Asia & Pacific", "Europe & Central Asia", "Latin America & Caribbean", "Middle East & North Africa", "South Asia", "Sub-Saharan Africa", "Low income", "Low & middle income", "Lower middle income", "Upper middle income", "High income")
```

Then, we rename those countries with inconsistent naming convention. Since we should only care about countries whose poverty headcount is available, reusing the list generated above, we can identify:

1.  Cote d'Ivoire (also Côte d'Ivoire)
2.  Czechia (also Czechoslovakia or Czech Republic)
3.  Curacao (also Curaçao)
4.  Turkiye (formerly known as Turkey, also Türkiye)
5.  Sao Tome and Principe (also São Tomé and Príncipe)

```{r}
# mapping standard name and variation
nameMap <- tibble(standard = c("Cote d'Ivoire", "Czechia", "Czechia", "Curacao", "Turkiye", "Turkiye", "Sao Tome and Principe"), variation = c("Côte d'Ivoire", "Czechoslovakia", "Czech Republic", "Curaçao", "Turkey", "Türkiye", "São Tomé and Príncipe"))

correctName <- function(name) {
  tibble(name = name) %>% 
    left_join(nameMap, by = c("name" = "variation")) %>% 
    mutate(standard = ifelse(is.na(standard), name, standard)) %>% 
    select(standard) %>% 
    pull()
}

orig.name <- c("Vietnam", "China", "Turkey", "Czechia Republic")
correctName(orig.name)
```

Let's test this out!

```{r}
d <- poverty.headcount %>%
  # correct name here
  mutate(country.name = correctName(country.name)) %>% 
  select(country.name) %>%
  mutate(inPov = T) %>%
  full_join(income.class %>% 
              # correct name here
              mutate(country.name = correctName(country.name)) %>% 
              select(country.name) %>% 
              mutate(inIncome = T), by = "country.name") %>%  
  full_join(region.class %>%
              # correct name here
              mutate(country.name = correctName(country.name)) %>% 
              select(country.name) %>% 
              mutate(inReg = T), by = "country.name") %>% 
  filter(!(country.name %in% spec.reg)) %>% 
  mutate(inPov = !is.na(inPov), inIncome = !is.na(inIncome), inReg = !is.na(inReg))

# countries not in region list, but is in Pov list
d %>% 
  filter(!inReg & inPov) %>% 
  distinct(country.name) %>% 
  nrow()
# countries not in income list, but is in Pov list
d %>% 
  filter(!inIncome & inPov) %>% 
  distinct(country.name) %>% 
  nrow()
```

We are *pretty* confident that there's no inconsistent naming left unprocessed in the data sets.

```{r}
# Rename countries in all data sets.
poverty.headcount <- poverty.headcount %>%
  mutate(country.name = correctName(country.name))
mpi <- mpi %>%
  mutate(country.name = correctName(country.name))
education.expenditure.total <- education.expenditure.total %>%
  mutate(country.name = correctName(country.name))
education.expenditure.primary <- education.expenditure.primary %>%
  mutate(country.name = correctName(country.name))
education.expenditure.secondary <- education.expenditure.secondary %>%
  mutate(country.name = correctName(country.name))
education.expenditure.tertiary <- education.expenditure.tertiary %>%
  mutate(country.name = correctName(country.name))
health.expenditure <- health.expenditure %>%
  mutate(country.name = correctName(country.name))
military.expenditure <- military.expenditure %>%
  mutate(country.name = correctName(country.name))
fdi <- fdi %>%
  mutate(country.name = correctName(country.name))
labour.force.participation <- labour.force.participation %>%
  mutate(country.name = correctName(country.name))
unemployment.rate <- unemployment.rate %>%
  mutate(country.name = correctName(country.name))
population.growth <- population.growth %>%
  mutate(country.name = correctName(country.name))
rural.population.growth <- rural.population.growth %>%
  mutate(country.name = correctName(country.name))
urban.population.growth <- urban.population.growth %>%
  mutate(country.name = correctName(country.name))
gdp.deflator <- gdp.deflator %>%
  mutate(country.name = correctName(country.name))
gender.equality <- gender.equality %>%
  mutate(country.name = correctName(country.name))
gross.capital.formation <- gross.capital.formation %>%
  mutate(country.name = correctName(country.name))
trade <- trade %>%
  mutate(country.name = correctName(country.name))
region.class <- region.class %>%
  mutate(country.name = correctName(country.name))
income.class <- income.class %>%
  mutate(country.name = correctName(country.name))
gdp.pc <- gdp.pc %>%
  mutate(country.name = correctName(country.name))
```

Join the data

```{r, message = F}
countries <- poverty.headcount %>% 
  # We used a full join here so we can conduct a separate analysis on mpi later
  full_join(mpi, by = c("country.name", "country.code", "year")) %>%
  left_join(income.class, c("country.name", "country.code", "year")) %>% 
  left_join(region.class, by = "country.name") %>%
  left_join(education.expenditure.total, by = c("country.name", "country.code", "year")) %>%
  left_join(education.expenditure.primary, by = c("country.name", "country.code", "year")) %>%
  left_join(education.expenditure.secondary, by = c("country.name", "country.code", "year")) %>%
  left_join(education.expenditure.tertiary, by = c("country.name", "country.code", "year")) %>%
  left_join(health.expenditure, by = c("country.name", "country.code", "year")) %>%
  left_join(military.expenditure, by = c("country.name", "country.code", "year")) %>%
  left_join(fdi, by = c("country.name", "country.code", "year")) %>%
  left_join(labour.force.participation, by = c("country.name", "country.code", "year")) %>%
  left_join(unemployment.rate, by = c("country.name", "country.code", "year")) %>%
  left_join(population.growth, by = c("country.name", "country.code", "year")) %>%
  left_join(rural.population.growth, by = c("country.name", "country.code", "year")) %>%
  left_join(urban.population.growth, by = c("country.name", "country.code", "year")) %>%
  left_join(gdp.deflator, by = c("country.name", "country.code", "year")) %>%
  left_join(gender.equality, by = c("country.name", "country.code", "year")) %>%
  left_join(gross.capital.formation, by = c("country.name", "country.code", "year")) %>%
  left_join(trade, by = c("country.name", "country.code", "year")) %>% 
  left_join(gdp.pc, by = c("country.name", "country.code", "year")) %>% 
  # filter special groups
  filter(!(country.name %in% spec.reg))
```

Data preview

```{r}
head(countries)
str(countries)
summary(countries)
```

There's no NA in `reg`, which is a sign that all naming in the data is remedied. There's some expected NAs in `income` and `pov`, as these data are collected by year. There's a substantial amount of missing data in `mpi`, as this is a relative new concept. We will address the nature, and processing of missing data in the next sections.

## 2.2. Missing values

As observed from the summary above, the data set contains a lot of missing values in some of the variables.

```{r}
mean(is.na(countries))
```

About 19% of the data set is missing.

```{r}
nCompleteObs <- sum(complete.cases(countries))
print(paste("No. of complete cases:", nCompleteObs))
```

There are only 3 complete cases where all the variable is available. This is nowhere near acceptable to conduct any meaningful analysis. Therefore, we need to eliminate some variables for a more balance data set.

```{r}
missings <- colMeans(is.na(countries))
ggplot(mapping = aes(x = names(missings), y = missings, fill = missings < 0.35)) +
  geom_bar(stat = "identity") +
  ggtitle("Missing rate") +
  xlab("variables") +
  ylab("% missing") +
  theme(axis.text.x = element_text(size=9, angle=90))

missings[missings > 0.35]
```

There are 5 variables with missing rate \>35%.

expenditure in primary, secondary, and tertiary edication can be very useful and relevant information to predict poverty reduction [@pubspend&pov]. However, we would like to exclude these variables from some first analyses to make use of the richer set of data. We can conduct a separate analysis with these variable to gain more insight.

```{r}
# variables with high missing rate
hMiss <- names(missings[missings > 0.35])
# exclude these variables in countries1
countries1 <- countries %>% 
  select(!hMiss) %>% 
  filter(!is.na(pov))
str(countries1)
```

Re-evaluate the `countries1` set.

```{r}
mean(is.na(countries1))
sum(complete.cases(countries1))
mean(complete.cases(countries1))
```

On average, each column has 6% missing rate, results in 937 complete data point (i.e. 49%). This can be a sufficient number for the analysis. However, the missing data can induce loss of power due to the reduced sample size, and some other biases depending on which variables is missing.

```{r}
# complete rate of data by regions
countries1 %>% 
  mutate(isComplete = complete.cases(.)) %>% 
  group_by(reg) %>% 
  summarise(complete.rate = mean(isComplete)) %>% 
  arrange(desc(complete.rate))
```

Countries from North America, Sub-Saharan Africa, and South Asia have the highest rate of missing data. We suspect that Sub-Saharan Africa, and South Asia are comparably less accessible regions. We also know that Americans don't like filling out forms, so their high rate of missing data is understandable as well.

Still, we need to find a way to address this issue. we propose several approaches:

1.  **Use complete cases**: Only use the complete cases for the analysis. This is a straightforward approach, but doesn't resolve the bias resulted from the mass loss of data.
2.  **Selectively remove variables with high missing rate**: The same as we did before, but this process should be carried out carefully as we run the chance of dropping an important variable.
3.  **Update the data set as we select variables**: As we drop insignificant variables (in backward selection), the number of NAs are changed as well. We can utilize the extra complete cases to build the next model in the steps.
4.  **Imputation**: The idea is to replace the missing observations on the response or the predictors with artificial values that try to preserve the data set structure. This is a quite complex topic of its own, but we think why not. You can read more from @imputation.

## 2.3. Descriptive Analytics
Distribution of the predicted variable `pov`
```{r}
ggplot(countries, aes(x = pov)) +
  geom_histogram(aes(y = ..density..), bins = 30) +
  geom_density(alpha = .2, fill = "blue")
```
The graph displays a decreasing rate as poverty indicator increasing. This might not be representative of the current state of poverty in the world, but of the number presented in our data. For example, more recent data is likely to be more inclusive than ancient data, when poverty is more prevalent. We should look at data from the same period.

```{r}
# number of data point available from 1967 to 2021
ggplot(poverty.headcount, aes(x = year)) +
  geom_histogram(bins = 30)

# pov data from 1998 and 2018
pov.98.18 <- poverty.headcount %>%
  filter(year == 1998 | year == 2018)
pov.98.18 %>% 
  group_by(year) %>% 
  summarise(sum = n())

ggplot(pov.98.18, aes(x = pov)) +
  geom_histogram(aes(y = ..density..), bins = 30) +
  geom_density(alpha = .2, fill = "blue") +
  facet_grid(cols = vars(year))
```

The graph for 1998 has a much gentler slope, meaning poverty was more popular during that time, as predicted from out intuition. What about the general progress of the world?

```{r}
# Re-import pov and only take special regions
# geographic
geo.regs <- c("EAS", "ECS", "LCN", "MEA", "SAS", "SSF", "WLD")
# economics
eco.regs <- c("HIC", "LIC", "LMC", "LMY", "UMC")

pov.reg <- importWDI("../data/poverty.headcount.215dollar.csv", "pov") %>% 
  filter(country.code %in% c(geo.regs, eco.regs)) %>% 
  mutate(type = ifelse(country.code %in% geo.regs, "Geographic", "Economics"))

pov.reg %>%
  distinct(country.code, country.name, type) %>% 
  arrange(type)

# World
ggplot(pov.reg %>% filter(country.code == "WLD"),
       aes(x = year, y = pov)) +
  geom_point() +
  geom_smooth(formula = y~x, method = loess)
```

An overall very steady decrease of poverty. How about each region?

```{r}
ggplot(pov.reg %>% filter(country.code != "WLD"),
       aes(x = year, y = pov, color = country.name)) +
  geom_point() +
  geom_smooth(formula = y~x, method = loess) +
  facet_grid(rows = vars(type))
```

There's a general steady, but distinct decline of poverty over time in each type region of respective type. Latin America & Caribbean, Europe & Central Asia, Middle East & North Africa, and High Income group has a more gradual decline as they are not very poor to begin with.  

Among the income groups, Low & Middle Income, Lower & Middle Income, and Upper Middle Income have quite similar in term of poverty indicator and slope over the year. While these values vary greatly among different geographical regions.  

Let's see some important statistics

```{r}
stats <- poverty.headcount %>% 
  summarise(count = n(), skewness = skewness(pov), kurtosis = kurtosis(pov), std.deviation = sd(pov))

kable(stats)
```

## 2.4. Data Source

- [poverty.headcount](https://data.worldbank.org/indicator/SI.POV.DDAY)
- [mpi](https://data.worldbank.org/indicator/SI.POV.MDIM.XQ)
- [education.expenditure.primary](https://data.worldbank.org/indicator/SE.XPD.PRIM.ZS)
- [education.expenditure.secondary](https://data.worldbank.org/indicator/SE.XPD.SECO.ZS)
- [education.expenditure.tertiary](https://data.worldbank.org/indicator/SE.XPD.TERT.ZS)
- [education.expenditure.total](https://data.worldbank.org/indicator/SE.XPD.TOTL.GD.ZS)
- [health.expenditure](https://data.worldbank.org/indicator/SH.XPD.GHED.GD.ZS)
- [military.expenditure](https://data.worldbank.org/indicator/MS.MIL.XPND.GD.ZS)
- [fdi](https://data.worldbank.org/indicator/BX.KLT.DINV.CD.WD)
- [unemployment.rate](https://data.worldbank.org/indicator/SL.UEM.TOTL.NE.ZS)
- [labour.force.participation](https://data.worldbank.org/indicator/SL.TLF.CACT.NE.ZS)
- [gender.equality](https://data.worldbank.org/indicator/IQ.CPA.GNDR.XQ)
- [population.growth](https://data.worldbank.org/indicator/SP.POP.GROW)
- [urban.population.growth](https://data.worldbank.org/indicator/SP.URB.GROW)
- [rural.population.growth](https://data.worldbank.org/indicator/SP.RUR.TOTL.ZG)
- [gdp.deflator](https://data.worldbank.org/indicator/NY.GDP.DEFL.KD.ZG)
- [gross capital formation](https://data.worldbank.org/indicator/NE.GDI.TOTL.ZS)
- [trade](https://data.worldbank.org/indicator/NE.TRD.GNFS.ZS)
- [region.class](https://datatopics.worldbank.org/world-development-indicators/the-world-by-income-and-region.html)
- [income.class](https://datahelpdesk.worldbank.org/knowledgebase/articles/906519-world-bank-country-and-lending-groups)
- [gross.capital.formation](https://data.worldbank.org/indicator/NE.GDI.TOTL.ZS)

# 3. Model Selection and Interpretation
## 3.1. Assumption Check
We can conduct some preliminary checks on linearity and correlation of predictors to have a better picture of the data. [Checklist](https://godatadrive.com/blog/basic-guide-to-test-assumptions-of-linear-regression-in-r)
**1. Linear relationship:**  
Use correlation matrix to check linearity
```{r, warning = FALSE}
# select only numeric data
countries.num <- countries %>% select(where(is.numeric))
chart.Correlation(countries.num, histogram = TRUE, pch = 19)
```
Which is utterly intelligible, but a majority of the fitted lines are linear at first glance. We should render separate graphs for the relationship between `pov` & other variables.
```{r}
# lengthen data table to variable-value pairs, with pov as predicted variable and others as predictive 
countries.num2 <- countries.num %>% 
  pivot_longer(cols = 3:ncol(.), names_to = "variable", values_to = "value") %>% 
  filter(!is.na(value) & !is.na(pov))

drawGraph <- function(indvar, data) {
  ggplot(data %>% 
           filter(variable == indvar),
         aes(x = value, y = pov)) +
    geom_point() +
    geom_smooth(method = lm) +
    labs(title = paste("Relationship pov ~", indvar),
         x = indvar,
         y = "pov")
}

ivs <- distinct(countries.num2, variable)[[1]]

for (indvar in ivs) {
  print(
    ggplot(countries.num2 %>% 
           filter(variable == indvar),
         aes(x = value, y = pov)) +
    geom_point() +
    geom_smooth(formula = y~x, method = lm) +
    labs(title = paste("Relationship pov ~", indvar),
         x = indvar,
         y = "pov")
  )
}
```

Most variables display linear relationship with some exceptions, which are more appropriate to assume a logarithmic relationship. Gender equality should be viewed as a categorical data.
```{r}
logIvs <- c("fdi", "gdp.dflt", "gdp.pc")

for (indvar in logIvs) {
  print(
    ggplot(countries.num2 %>% 
           filter(variable == indvar),
         aes(x = log(value), y = pov)) +
    geom_point() +
    geom_smooth(method = lm) +
    labs(title = paste0("Relationship pov ~ log(", indvar, ")"),
         x = paste0("log(", indvar, ")"),
         y = "pov")
  )
}
```

The relationship are more linear after the transformation.

```{r}
gdr.eql <- countries.num2 %>% 
  filter(variable == "gdr.eql") %>% 
  mutate(value = factor(value))

ggplot(gdr.eql,
       aes(x = value, y = pov)) +
  geom_boxplot() +
  geom_smooth(method = lm) +
  labs(title = paste("Relationship pov ~ gdr.eql"),
       x = "gdr.eql",
       y = "pov")
```
There's a significant correlation between gender equality and poverty.  
Correlation coefficients between `pov` and predictors.

```{r}
# transform some variables
countries.num3 <- countries.num %>% 
  mutate(lfdi = log(fdi), lgdp.dflt = log(gdp.dflt), lgdp.pc = log(gdp.pc)) %>% 
  mutate(lfdi = ifelse(is.nan(lfdi) | lfdi == -Inf, NA, lfdi))

# name of the independent variables
cn <- colnames(countries.num3)[-c(1,2)]

# correlation of independent variable and pov
corWithPov <- function(indevar, data) {
  cor(data[[indevar]], data[["pov"]], use = "complete.obs")
}

cor.pov <- sapply(cn, corWithPov, data = countries.num3)
kable(cor.pov)
```

`edu.ter`, `mil`, `fdi`, `lbr.part`, `unemp`, `gdp.dflt`, `gcf` have negligible correlation with `pov`. `lgdp.pc`, `lfdi`, and `lgdp.dflt` have stronger linear relationship with `pov` than their un-transformed counterparts.

```{r}
countries1 <- countries1 %>% 
  mutate(lfdi = log(fdi), lgdp.dflt = log(gdp.dflt), lgdp.pc = log(gdp.pc)) %>% 
  mutate(lfdi = ifelse(is.nan(lfdi) | lfdi == -Inf, NA, lfdi))
```


**2. Predictors are independent**
There should be no correlation/multicolinearity between each pair of predictors.

```{r}
countries.arr <- simplify2array(countries.num %>% 
                                  select(-c("year", "pov")))

cor.mtx <- rcorr(countries.arr, type = "spearman")

round(cor.mtx$r, 2)

corrplot(cor.mtx$r, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)
```
Some significant correlations can be found between `gdr.eql` with `edu.pri`, `pop.gwth.total`, and `fdi` with `mil`, etc. We expect these variables to be eliminated in the VIF test. This might be a good property for imputation (3.2.4)

## 3.2. Ordinary Multiple Linear Regression

We conduct a normal linear regression, following the approaches mentioned above to address missing values issues.

### 3.2.1. Use Complete Cases (To be done)
#### 3.2.1.1. Model Fitting
#### 3.2.1.2. Assessment
#### 3.2.1.3. Interpretation

### 3.2.2. Selectively remove variables with high missing rate (To be done)
#### 3.2.2.1. Model Fitting
#### 3.2.2.2. Assessment
#### 3.2.2.3. Interpretation

### 3.2.3. Update the data set as we select variables (To be done)
#### 3.2.3.1. Model Fitting
#### 3.2.3.2. Assessment
#### 3.2.3.3. Interpretation

### 3.2.4. Imputation
**1. What is imputation**  
Imputation is a technique to handle missing values by replacing missing data with substitute values. In our study, we focus on "item imputation", which means substituting for a component of a data point (i.e. a variable). The general idea is to take a value that preserve the property of the data (e.g., distribution, mean, standard deviation), and use other variables to predict the missing values (much like regression). Since we observed some correlations during descriptive analysis, we expect the imputation algorithm to work well.

We use **Multivariate Imputation By Chained Equations (MICE)** as primary tool for this technique, because of its wide acceptance in scientific studies (@imputWorks). In order to use this technique, we have to assume that the "missingness" of a field can be explained by the values in other columns, (e.g., If the countries is in North America, it's more likely to be missing as we have discovered in 2.2). The general ideas of the algorithm is to fill in the missing values, and improve it iteratively until predicted values converse to a stable point. The algorithmic details of MICE is very concisely (and enthrallingly) explained in @micealgo.  

We use relatively small parameters in the interest of time.

- `m = 5` imputed data sets
- `maxit = 30` iterations

The imputation method is `cart` (Classification and Regression Trees).

```{r}
set.seed(1984)
# split data
isComplete <- which(complete.cases(countries1))
idx <- sample(isComplete, replace = F, 0.3 * nrow(countries1))
countries.train.4 <- countries1[-idx,]
countries.test.4 <- countries1[idx,]

# blank for no imputation, cart for variable requiring imputation
meth <- c(rep("", 4), "cart", "", rep("cart", 16))
names(meth) <- colnames(countries1)
meth

# run the algorithm
# countries1.imputed <- mice(countries1, m = 3, maxit = 20, method = meth)
# saveRDS(countries1.imputed, 'countries1.imputed.RData')
countries1.imputed <- readRDS('countries1.imputed.RData')
summary(countries1.imputed)
```
We can take a look into one of the imputated data sets.
```{r}
countries1.imputed.1 <- complete(countries1.imputed, 1)
summary(countries1.imputed.1)
sum(!complete.cases(countries1.imputed.1))
```
We found no missing cases as expected.

#### 3.2.4.1. Model Fitting
##### A. Ordinary Linear Regression
We generated 3 sets of imputated (training) data. We can build separate models using each data set, and combine the estimates using [pooling rule](https://amices.org/mice/reference/pool.html#details). With all the generated data sets.

```{r}
formula.str <- 'pov ~ country.code+year+income+edu.total+hlth+mil+fdi+lbr.part+unemp+pop.gwth.total+pop.gwth.rural+pop.gwth.urban+gdp.dflt+gcf+trade+gdp.pc+lfdi+lgdp.dflt+lgdp.pc'

# Using single imputed data set
fit1 <- lapply(1:countries1.imputed$m, \(i) lm(as.formula(formula.str), data = complete(countries1.imputed, i)))

# Build models with all generated data set and pool the estimates
fit2 <- with(data = countries1.imputed, exp = lm(as.formula(formula.str)))
fit2.combined <- pool(fit2)
# dummy lm model
fit2.dummy <- lm(as.formula(formula.str), data = complete(countries1.imputed, 1))
# replace coefficients of dummy model
name.coef <- names(fit2.dummy$coefficients)
fit2.dummy$coefficients <- fit2.combined$pooled$estimate
names(fit2.dummy$coefficients) <- name.coef
```
Helper function to calculate R-Squared
```{r}
r2 <- function(pred, orig) {
   RSS <- sum((pred - orig)^2)
   TSS <- sum((orig - mean(orig))^2)
   R2 <- 1 - RSS/TSS
   return (R2)
}

adjR2 <- function(pred, orig, k) {
  R2 <- r2(pred, orig)
  n <- length(pred)
  adjr2 <- 1 - (1-R2)*(n-1)/(n-k-1)
  return (adjr2)
}
```
Adjusted R-Squared on train and test
```{r}
fit1.summ <- lapply(fit1, \(f) {
  # number of variables
  k <- length(f$coefficients) - 1
  # predict value of test
  pred <- predict(f, countries.test.4)
  resid <- pred - countries.test.4$pov
  data.frame(
    `Train MSE` = mean(summary(f)$residuals^2),
    `Test MSE` = mean(resid^2),
    `Train MAE` = mean(abs(summary(f)$residuals)),
    `Test MAE` = mean(abs(test.resid)),
    `Train Adj.R2` = summary(f)$adj.r.squared,
    `Test Adj.R2` = adjR2(pred, countries.test.4$pov, k)
  )
})

# number of variables
k <- length(fit2.dummy$coefficients) - 1
# number of variables
fit2.pred <- predict(fit2.dummy, countries.test.4)
resid <- fit2.pred - countries.test.4$pov
fit2.summ <- data.frame(
  `Train MSE` = mean(summary(fit2.dummy)$residuals^2),
  `Test MSE` = mean(resid^2),
  `Train MAE` = mean(abs(summary(fit2.dummy)$residuals)),
  `Test MAE` = mean(abs(test.resid)),
  `Train Adj.R2` = pool.r.squared(fit2, adjusted = T)[,'est'],
  `Test Adj.R2` = adjR2(fit2.pred, countries.test.4$pov, k)
)

res <- do.call(rbind.data.frame, fit1.summ)
res <- rbind(res, fit2.summ)
res <- cbind(data.frame(Set = c(1:3, "Pooled")), res)

kable(res)
```

Pooled model slightly improve performance. Test data fitting has better MSE and MAE than train data. However, Adjusted R-Squared is lower in test data. There might be some over-fitting in our models.

```{r}
# find coef of variable other than country.code
var <- c("year","incomeL","incomeLM","incomeUM","edu.total","hlth","mil","fdi","lbr.part","unemp","pop.gwth.total","pop.gwth.rural","pop.gwth.urban","gdp.dflt","gcf","trade","gdp.pc","lfdi","lgdp.dflt","lgdp.pc")

res <- do.call(rbind, lapply(fit1, \(f) f$coefficients[var]))
res <- rbind(res, fit2.dummy$coefficients[var])
res <- cbind(data.frame(Set = c(1:3, "Pooled")), res)
kable(res)
```

In contradiction to expectation, `hlth` is positively correlated with `pov`, and lower-middle income countries (incomeLM) are less poor than high income countries (incomeH). `pop.gwth.total` is negatively related to `pov`, while `pop.gwth.rural` and `pop.gwth.urban` are positively related. They might be indications of over-fitting model.

```{r}
# fit model on imputated set no. 1
summary(fit1[[1]])
```

There are also some weak variable, we should perform some variable selection.

##### B. VIF

Our data set contains a lots of variables. We can perform some variable selection to reduce over-fitting.

```{r}
gvif <- lapply(fit1, vif)
gvif
```

[Some forum](https://stats.stackexchange.com/questions/70679/which-variance-inflation-factor-should-i-be-using-textgvif-or-textgvif) suggested that we should use the standard $GVIF^{1/(2 \cdot df)} < 2$ as equivalent to $GVIF < 4$ to account for high degree of freedom of some variables.

```{r}
# adjusted gvif higher than 3
lapply(gvif, \(g) {
  g[g[,3] > 3, 3]
})
```

Remove the highest-ranked variables (`pop.gwth.total`, `pop.gwth.urban`, `pop.gwth.rural`, `gdp.pc`, `lgdp.pc`) and rebuild the models.

```{r}
formula.str <- 'pov ~ country.code+year+income+edu.total+hlth+mil+fdi+lbr.part+unemp+gdp.dflt+gcf+trade+lfdi+lgdp.dflt'

# Using single imputed data set
fit1 <- lapply(1:countries1.imputed$m, \(i) lm(as.formula(formula.str), data = complete(countries1.imputed, i)))
```

Re-run GVIF analysis

```{r}
gvif <- lapply(fit1, vif)

lapply(gvif, \(g) {
  g[g[,3] > 2, 3]
})
```

There're still some variable with adjusted GVIF > 2. We are interested in the effects of `hlth` (expenditure in Healthcare), and `mil` (expenditure in Military), so we won't remove those variables. We can remove `trade`.

```{r}
# remove trade
formula.str <- 'pov ~ country.code+year+income+edu.total+hlth+mil+fdi+lbr.part+unemp+gdp.dflt+gcf+lfdi+lgdp.dflt'

# Using single imputed data set
fit1 <- lapply(1:countries1.imputed$m, \(i) lm(as.formula(formula.str), data = complete(countries1.imputed, i)))

# gvif
gvif <- lapply(fit1, vif)

lapply(gvif, \(g) {
  g[g[,3] > 2, 3]
})
```

The adjusted GVIF are acceptably low.

Coefficients overview.

```{r}
# find coef of variable other than country.code
var <- c("year","incomeL","incomeLM","incomeUM","edu.total","hlth","mil","fdi","lbr.part","unemp","gdp.dflt","gcf","lfdi","lgdp.dflt")

res <- do.call(rbind, lapply(fit1, \(f) f$coefficients[var]))
kable(res)
```

There are several sign-switching in `income` coefficients, indicating the effect of removing some multicollinearity.

```{r}
fit1.summ <- lapply(fit1, \(f) {
  k <- length(f$coefficients) - 1
  pred <- predict(f, countries.test.4)
  data.frame(
    `Train Adj.R2` = summary(f)$adj.r.squared,
    `Test Adj.R2` = adjR2(pred, countries.test.4$pov, k)
  )
})

res <- do.call(rbind.data.frame, fit1.summ)
res <- cbind(data.frame(Set = 1:3), res)
kable(res)
```

Directly removing variables seems to be penalising our test results. As our interest is to investigate the effect of certain variables on poverty, the yielded R-Squared is within acceptable range. We can continue variable selection on the new models.

```{r}
formula.str <- 'pov ~ country.code+year+income+edu.total+hlth+mil+fdi+lbr.part+unemp+gdp.dflt+gcf+lfdi+lgdp.dflt'
```


##### B. Step-wise AIC
We can conduct a step-wise AIC variable selection. It is similar to the procedure we use in class, but based on a metric call AIC (Akaike Information Criterion), which is an estimator of prediction error and relative quality of statistical models. The lower AIC is, the better the model fits. \\

```{r}
# helper
fitAIC <- function(i) {
  set.seed(1984)
  fit <- lm(as.formula(formula.str), complete(countries1.imputed, i))
  aic.fit <- stepAIC(fit, trace = T, direction = 'backward')
  return(aic.fit)
}

# build models
aic.fits <- lapply(1:countries1.imputed$m, fitAIC)

# check final terms of each model
lapply(aic.fits, \(mod) formula(mod$terms))
```

The most commonly removed variables are:

- `gdp.dflt`
- `ldgp.dflt`
- `unemp`
- `fdi`

```{r}
# predict with each model
aic.pred <- lapply(aic.fits, predict, newdata = countries.test.4)
# calculate adj r2
aic.train.r2 <- unlist(lapply(aic.fits, \(model) summary(model)$adj.r.squared))

k <- lapply(aic.fits, \(m) length(m$coefficients))
aic.test.r2 <- unlist(mapply(adjR2, aic.pred, k = k, MoreArgs = list(orig = countries.test.4$pov)))

res <- data.frame(`set no.` = 1:3, train = aic.train.r2, test = aic.test.r2)
```

We can compare Adjusted R-Squared to estimate relative over-fitting in the new models.

```{r}
kable(res)
```

Performance on train set and test set are not affected. However, we have simplified the model by removing some unnecessary terms. We can update our models.

```{r}
formula.str <- 'pov ~ country.code+year+income+edu.total+hlth+mil+lbr.part+gcf+lfdi'

fit3 <- lapply(1:countries1.imputed$m, \(i) lm(as.formula(formula.str), data = complete(countries1.imputed, i)))

# Build models with all generated data set and pool the estimates
fit4 <- with(data = countries1.imputed, exp = lm(as.formula(formula.str)))
fit4.combined <- pool(fit4)
# dummy lm model
fit4.dummy <- lm(as.formula(formula.str), data = complete(countries1.imputed, 1))
# replace coefficients of dummy model & predict
fit4.dummy$coefficients <- fit4.combined$pooled$estimate

fit3.summ <- lapply(fit3, \(f) {
  k <- length(f$coefficients) - 1
  pred <- predict(f, countries.test.4)
  resid <- pred - countries.test.4$pov
  data.frame(
    `Train MSE` = mean(summary(f)$residuals^2),
    `Test MSE` = mean(resid^2),
    `Train MAE` = mean(abs(summary(f)$residuals)),
    `Test MAE` = mean(abs(test.resid)),
    `Train Adj.R2` = summary(f)$adj.r.squared,
    `Test Adj.R2` = adjR2(pred, countries.test.4$pov, k)
  )
})

k <- length(fit4.dummy$coefficients) - 1
fit4.pred <- predict(fit4.dummy, countries.test.4)
resid <- fit4.pred - countries.test.4$pov
fit4.summ <- data.frame(
  `Train MSE` = mean(summary(fit4.dummy)$residuals^2),
  `Test MSE` = mean(resid^2),
  `Train MAE` = mean(abs(summary(fit4.dummy)$residuals)),
  `Test MAE` = mean(abs(test.resid)),
  `Train Adj.R2` = pool.r.squared(fit4, adjusted = T)[,'est'],
  `Test Adj.R2` = adjR2(fit4.pred, countries.test.4$pov, k)
)

res <- do.call(rbind.data.frame, fit3.summ)
res <- rbind(res, fit4.summ)
res <- cbind(data.frame(Set = c(1:3, "Pooled")), res)

kable(res)
```

Coefficients overview.

```{r}
var <- c("year","incomeL","incomeLM","incomeUM","edu.total","hlth","mil","lbr.part","gcf","lfdi")

res <- do.call(rbind, lapply(fit3, \(f) f$coefficients[var]))
kable(res)
```

We can further treat over-fitting with regularization.

##### C. Regularization

Helper functions to standardise data.

```{r}
sd0 <- function(vct) {
  if (!is.numeric(vct)) {
    return(NA)
  }
  
  return(sd(vct, na.rm = T))
}


std0 <- function(vct, scl) {
  if (!is.numeric(vct)) {
    return(vct)
  }
  
  return(vct/scl)
}

standardise <- function(train, test) {
  # save number of train and combine both data set
  trainCases <- 1:nrow(train)
  data <- rbind(train, test)
  # calc scaler
  scaler <- unlist(lapply(data, sd0))
  # get the numeric columns
  numCols <- which(unlist(lapply(data, is.numeric)))
  # divide numeric columns by scalers, and combine with factor columns
  num <- as.data.frame(mapply(std0, vct = data[,numCols], scl = scaler[numCols], SIMPLIFY = T))
  fct <- data[,-numCols]
  final <- cbind(fct, num)
  # split to train and test
  trainSet <- final[trainCases,]
  testSet <- final[-trainCases,]
  
  res <- list(final = final, train = trainSet, test = testSet, scaler = scaler)
  return(res)
}
```

Helper function to build the optimal model.

```{r}
optimalModel <- function(formula, train, test) {
  set.seed(1984)
  # standise and save number of train cases (to split later)
  std.data <- standardise(train, test)
  trainCases <- 1:nrow(train)
  # matrix-ise
  xs <- model.matrix(formula, std.data$final)[,-1]
  ys <- std.data$final$pov
  # split data
  train.x <- xs[trainCases,]
  train.y <- ys[trainCases]
  test.x <- xs[-trainCases,]
  test.y <- ys[-trainCases]
  # list of tested alpha, resolution = 0.1
  alphas <- seq(0, 1, 0.1)
  # build a bunch of models
  models <- lapply(alphas, \(a) cv.glmnet(train.x, train.y, type.measure = "mse", alpha = a))
  cv.error <- unlist(lapply(models, \(model) model$cvm[model$lambda == model$lambda.min]))
  # best model
  best.model.idx <- which.min(cv.error)
  # optimal alpha and lambda
  alpha.opt <- alphas[best.model.idx]
  lambda.opt <- models[[best.model.idx]]$lambda.min
  best.model <- glmnet(train.x, train.y, alpha = alpha.opt, lambda = lambda.opt)
  # predict for test data
  train.fit <- predict(best.model, train.x)
  test.fit <- predict(best.model, test.x)
  # evaluation - train
  k <- best.model$df
  train.r2 <- best.model$dev.ratio
  train.adjR2 <- adjR2(train.fit, train.y, k)
  train.resid <- train.fit - train.y
  train.mse <- mean(train.resid^2)
  train.rmse <- sqrt(train.mse)
  train.mae <- mean(abs(train.resid))
  train.mape <- mean(abs(train.resid / train.y))
  # evaluation - test
  test.r2 <- r2(test.fit, test.y)
  test.adjR2 <- adjR2(test.fit, test.y, k)
  test.resid <- test.fit - test.y
  test.mse <- mean(test.resid^2)
  test.rmse <- sqrt(test.mse)
  test.mae <- mean(abs(test.resid))
  test.mape <- mean(abs(test.resid / test.y))
  # final results
  results <- list(
    train = list(
      data = train,
      r2 = train.r2,
      adj.r2 = train.adjR2,
      residuals = train.resid,
      mse  = train.mse,
      rmse = train.rmse,
      mae  = train.mae,
      mape = train.mape
    ),
    test = list(
      data = test,
      r2 = test.r2,
      adj.r2 = test.adjR2,
      residuals = test.resid,
      mse  = test.mse,
      rmse = test.rmse,
      mae  = test.mae,
      mape = test.mape
    ),
    scaler = std.data$scaler,
    alpha = alpha.opt,
    lambda = lambda.opt,
    model = best.model
  )
  
  return(results)
}
```

Build the optimal models using each imputated data set.

```{r}
# helper function to build from a specific imputated data
buildWith <- function(set, imputed, test, formula, fun) {
  train <- complete(imputed, set)
  model <- fun(formula, train, test)
  return(model)
}

set.seed(1984)

models <- lapply(1:countries1.imputed$m, buildWith, imputed = countries1.imputed, test = countries.test.4, formula = as.formula(formula.str), fun = optimalModel)
```

Display parameters and evaluation.

```{r}
result.list <- lapply(models, \(mod) {
  data.frame(
    alpha = mod$alpha,
    lambda = mod$lambda,
    `Train MSE` = mod$train$mse,
    `Test MSE` = mod$test$mse,
    `Train MAE` = mod$train$mae,
    `Test MAE` = mod$test$mae,
    `Train R2` = mod$train$r2,
    `Test R2` = mod$test$r2,
    `Train Adj.R2` = mod$train$adj.r2,
    `Test Adj.R2` = mod$test$adj.r2
  )
})

result.df <- do.call(rbind.data.frame, result.list)
result.df <- cbind(data.frame(Set = 1:countries1.imputed$m), result.df)
kable(result.df)
```

There are some improvement on the test data performance based on adjusted R-Squared. MSE and MAE remain low on both data sets.

Coefficients overview.

```{r}
var <- c("year","incomeL","incomeLM","incomeUM","edu.total","hlth","mil","lbr.part","gcf","lfdi")

res <- do.call(rbind, lapply(models, \(f) coefficients(f$model)[var,]))
kable(res)
```

All non-country.code variables are non-zero. We can observe that the regularisation models just remove the effect of `country.code`, for countries that have similar "baseline".

#### 3.2.4.2. Assessment
Check assumption
outliers
#### 3.2.4.3. Interpretation

## 3.3. Panel Data Analysis (To be done)

# 4. Conclusion
# 5. Appendix
# 6. References
